[{"content":"Random Forest Random Forest works as a large collection of decorrelated decision trees and is based on bagging (boosted aggregating).\nA random sample is taken from the population with random variables (feature selection)A decision tree is made based on the sample.Multiple samples are taken with replacement (bootstrap sampling)Multiple decision trees are created based on their respective samplesAll the decision trees are used to create a ranking of classesThe final model is based on the most no votes for the class. In case of regression, it takes the average of outputs of different trees.\nAdvantages:Capable of performing both regression and classificationPerforms dimension reduction and handles missing values / outliers effectively.Can handle huge number of variablesShows Importance of variableRF is different from bagging : RF selects a subset of predictors as well. This results in decorrelated trees.\nDisadvantages:Good at classification but doesn‚Äôt provide precise continuous nature predictions for regression Black Box - very little control on what the model does. Only model parameters can be tuned.\n","description":"Random Forest","id":0,"section":"handbook","tags":null,"title":"Random Forest","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/"},{"content":"Logistic Regression Method = Maximum Likelihood Estimation / Chi-square Logistic Regression is used to model the probability of an outcome. It is based on concept of Generalized Linear Model (GLM).\nDependent Variable Y = Binary (eg 0 or 1)\nIndependent Variable X = Continuous or Categorical\nSince the dependent variable is binary, errors will be non-normally distributed. Also, Errors are heteroskedastic\nTypes:  Binary / Dichotomous - Dependent Variable is binary in nature (eg. 0/1) Ordered - Dependent Variable has ordered levels eg High, Medium, \u0026amp; Low Multinomial - Dependent Variable is nominal with more than two levels  Assumptions:  Binary logistic regression requires the dependent variable to be binary coded Model should have little or no multicollinearity Model should be fitted correctly: Neither overfitting or underfitting should occur The error terms should be independent ie the data should not be before-after samples Requires comparatively larger data sample (min 30 observations) There should be no outliers. Assessed by converting predictors to standardized or z scores and remove values below or greater than -3.29 or 3.29  Technique: It is similar to linear regression, except the Y variable is not regressed directly, instead the log odds ratio of Y is regressed.\nLogit is a log of odds and odds are a function of P.\n   Step Type Range     a Linear regression -‚àû to +‚àû   b Probability values 0 to 1   c Odds ratio 0 to ‚àû   d Log odds ratio -‚àû to +‚àû    (natural) Log of odds is taken for better expressing the results :\neg: odds of 90% and 10% expressed as :: (0.9/0.1) = 9 and conversely, (0.1/0.9) = 0.11\nHowever ln(0.9/0.1) = 2.217 and conversely, ln(0.1/0.9) = -2.217 relates in a much better way.\nInterpretation: Logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable\nImplementation from sklearn.linear_model import LogisticRegression model = LogisticRegression(solver='lbfgs', max_iter=500) # Instantiate model.fit(X_train, y_train)\t# Fit model  Output  Null Deviance : Indicates the response predicted by a model with nothing but an intercept.Lower the value, better the model. The difference between null and residual deviance should also be high Residual Deviance : Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model Fisher\u0026rsquo;s score : how far the model had to reiterate to get to the results, similar to AIC value.  Validation  Same significant variables should come in both the training and validation sample. The behavior of variables should be same in both the samples (same sign of coefficients) Beta coefficients should be close in training and validation samples KS statistics should be in top 3 deciles KS statistics should be between 40 and 70 Rank Ordering - There should not be any break in rank ordering. Lift Curve - The larger the cumulative lift value the better the accuracy Goodness of Fit Tests - Model should fit the data well. Check Hosmer and Lemeshow Test and Deviance and Residual Test.  Loss Function  A loss function is a measure of fit between a mathematical model of data and the actual data. Parameters of the model are chosen that minimize the badness of fit or maximize the goodness of fit of the model to the data With least squares, minimize SSres, the sum of squares residual and maximize the SSreg the sum of squares due to regression. With the logistic curve there is no mathematical solution that will produce the least squares estimates of the parameters. It\u0026rsquo;s more of an optimization problem For many of these models, the loss function chosen is called maximum likelihood  A likelihood is a conditional probability (eg P(Y|X), the probability of Y given X).\n","description":"Logistic Regression","id":1,"section":"handbook","tags":null,"title":"Logistic Regression","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/1-logistic-regression/"},{"content":"Measures of Central Tendency Mean The sum total of units divided by the number of units. Average\nŒº (population) | xÃÑ (sample) | 1/ni=1nxi\nMedian The middle / midpoint value in a sorted sequence. Middle\nIt divides the data 50(more):50(less).\nMode The most commonly occurring value Frequent\nTwo modes = Bimodal ; Multiple modes = Multimodal\n¬† Q) Why do we use mean most times ?\nA) It takes all values into consideration. Mode/median ignores\n  Q) When do we use median ?\nA) It is less prone to outliers as it doesn‚Äôt get affected by them.\n ¬†Nomenclature\n   Population Sample     X = set of population elements x = set of sample elements   N = population size n = sample size   Œº = population mean xÃÑ = sample mean   œÉ = population std dev s = sample std dev    ¬†Normal Distribution Data\n   Percentage Œº ¬± s.d.     68.3 % Œº ¬± 1œÉ   95.5 % Œº ¬± 2œÉ   99.7 % Œº ¬± 3œÉ    ¬†Population - our entire group of interest\nParameter - numeric summary about a population\nSample - subset of the population\nStatistic - numeric summary about a sample\n","description":"Central Tendency","id":2,"section":"handbook","tags":null,"title":"Central Tendency","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/"},{"content":"Linear Regression Linear Regression is the most basic type of regression that there is. It takes a variable and states that on the basis of other variables, it will predict the concerned variable. It does this by simply drawing a line through the points and generating an equation.\nMethod = Ordinary Least Square\n‚ö†Ô∏è Linear Regression requires the complete data should be numerical in nature.\n Dependent Variable Y: the one you need to predict Independent Variable X: the others with which you will predict Residual: difference between observation and the fitted line Error: Residual   The objective is to minimize the sum of squares of the residuals (difference between observation and the fitted line)\n Assumptions  Linear relationship between dependent \u0026amp; independent variables No presence of outliers Independent variables are independent of each other (non colinear) Errors, also called residuals  Should have constant variance (homoscedasticity) Are independent and identically distributed (iid) ie No Autocorrelation Are normally distributed with a mean of 0    Tests for Assumptions: Linearity  Methods :  Residuals vs Predicted plot / Residuals vs Actuals plot   Corrections :  Log transformation for strictly positive variables Adding regressor which is non-linear function eg x and x2 Create new variable which is sum/product of A \u0026amp; B    Multicollinearity  Methods:  Correlation Matrix VIF (Variance Inflation Factor)     VIF is calculated only on the Independent variables. It runs a series of auxiliary regressions which fetches the R2 value of Xi against other IVs.\nEg : If X2, X3, X4, have high R2 value when regressed against X1, it essentially means that X2, X3, X4 can explain a high amount of variation in X1 and it is redundant. Range = 1 to ‚àû 1 \u0026lt; low \u0026lt; 5 \u0026lt; medium \u0026lt; 10 \u0026lt; high\n Homoscedasticity  Methods:  Goldfeld-Quandt test Scatter plot (residuals vs predicted)   Corrections :  Take actual or predicted values of DV and plot it against errors. The plot should be random. If there is a trend, take log of DV.    Autocorrelation  Durbin-Watson Test : Tests for serial correlations between errors\nRange : 0-4 positive \u0026lt; 2 (uncorrelated) \u0026lt; negative  Multivariate Normal  Methods:  Kolmogorov-Smirnov test / Shapiro-Wilk / Anderson-Darling / Jarque-Bera Q-Q Plot Histogram with fitted normal curve   Corrections:  Nonlinear / Log transformation    Implementation from  Outputs and Validation  R-squared: The most common validation check Adjusted R-squared  One Hot Encoding This method is used to convert Categorical variables to Continuous variables. It is very simple:\n   Transport \u0026raquo; Car Bus Train     Car \u0026raquo; 1 0 0   Bus \u0026raquo; 0 1 0   Train \u0026raquo; 0 0 1    Dummy Variable Trap Lookout for this when converting categorical variables to continuous variables by one hot encoding (flag variable)\n [x] Include one less variable when adding dummy variables to regression. [x] The excluded variable serves as the base variable. [x] All the other values are a reference to the base variable.  ","description":"Linear Regression","id":3,"section":"handbook","tags":null,"title":"Linear Regression","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/"},{"content":"Easy to Overlook these   Multivariate Analysis is nothing but regression\n  Logistic regression IS a binomial regression (with logit link), a special case of the Generalized Linear Model. It doesn\u0026rsquo;t classify anything unless a threshold for the probability is set. Classification is just its application.\n  Stepwise regression is by no means a regression. It\u0026rsquo;s a (flawed) method of variable selection.\n  OLS is a method of estimation (among others: GLS, TLS, (RE)ML, PQL, etc.), NOT a regression.\n  Ridge, LASSO - it\u0026rsquo;s a method of regularization, NOT a regression.\n There are tens of models for the regression analysis. You mention mainly linear and logistic - it\u0026rsquo;s just the GLM! Learn the others too (link in a comment). STOP with the \u0026ldquo;17 types of regression every DS should know\u0026rdquo;.BTW, there\u0026rsquo;re 270+ statistical tests. Not just t, chi2 \u0026amp; Wilcoxon  ","description":"The Important Stuff Everyone Misses","id":4,"section":"information","tags":null,"title":"The Important Stuff Everyone Misses","uri":"https://do2blehelix.github.io/the-ml-handbook/information/important-stuff-everyone-misses/"},{"content":"Decision Trees It\u0026rsquo;s a type of supervised learning algorithm mostly used for classification problems. Works for both categorical and continuous I/P O/P variables.\nThe population is split into two or more homogeneous sets based on the splitting criterion.\nAnalogy : They ask a bunch of questions to arrive at a particular answer. (20 Questions)\nTypes :\n Categorical Variable Decision Tree : Categorical Target Variable Continuous Variable Decision Tree : Continuous Target Variable  Terminology :  Root Node : It represents the entire population / sample Parent and Child Node: A node, which is divided into sub-nodes is called parent node whereas sub-nodes are the child of parent node. Leaf / Terminal Node : Nodes that do not spit any further. Branch / Sub-tree : Sub section of the entire tree. Splitting : Process of dividing a node into two or more sub-nodes. Pruning : The process of removing sub-sections of a tree. Grafting : The process of adding a whole section to the tree.  Advantages :  Glass-box model. Patterns easy to understand and see. Useful in Data exploration : fastest way to find most significant variables and relation between variables. new variables can also be created with better prediction power. Less data cleaning required : not influenced by missing values and outliers to a fair degree Data type is not a constraint Non Parametric Method : no assumptions  Disadvantages :  Over fitting : If no limit is set for a decision tree, it will give 100% accuracy on training set Not fit for continuous variables : tends to lose information when bucketed into categories.  Implementation 1 2 3 4  from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier() # Instantiate model.fit(X_train, y_train) # Fit model   Hyperparameter (Tuning) :  Maximum Depth : The maximum number of levels in the tree Minimum number of samples to split : minimum required samples in node in order for it to further split Minimum number of samples per leaf : If it\u0026rsquo;s an integer, it\u0026rsquo;s the minimum number of samples required in a leaf. If it\u0026rsquo;s a float, it\u0026rsquo;s the minimum percentage of samples required in a leaf  Constraints :\nOverfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give you 100% accuracy on the training set because in the worst case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\nSetting constraints = pre pruning\nTree pruning = post pruning\n Setting constraints  Minimum samples required for a node split Minimum samples at a terminal node (leaf) Maximum depth of tree (vertical depth) Maximum number of terminal nodes Maximum features to consider for split   Tree Pruning  We first make the decision tree to a large depth. Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top. Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves.    Validation :   SSE\n  Classification/confusion matrix\n  Metrics: Gini Index [Categorical]\nThe gini measure gives the probability that 2 items chosen from the same population at random are in the same class. For a pure population this probability is 1.\n It works with categorical target variable ‚ÄúSuccess‚Äù or ‚ÄúFailure‚Äù. It performs only Binary splits Higher the value of Gini, higher the homogeneity. CART (Classification and Regression Tree) uses Gini method to create binary splits.  Gini Score :\nS1 : [ (a √∑ t1 )2 + (b √∑ t1 )2 ] = g1\nS2 : [ (a √∑ t2 )2 + (b √∑ t2 )2 ] = g2\nT = t1 + t2 = total 1 + total 2\nFinal gini score = [ g1 x (t1/T) ] + [ g2 x (t2/T) ]\nChi-Square [Categorical]\nIt is used to find the statistical significance of the differences between sub nodes and parent node. Measured by the sum of squares of standardized differences between observed and expected frequencies of target variable.\n It works with categorical target variable ‚ÄúSuccess‚Äù or ‚ÄúFailure‚Äù. It can perform two or more splits. Higher the value of Chi-Square, higher the statistical significance of differences between sub-node and Parent node or better the split. It generates tree called CHAID (Chi-square Automatic Interaction Detector)![](https://lh3.googleusercontent.com/9Q1JXlB5IUqr33_g3XflcYXaTtyddWv9aV3xMGqEFZ_9-XgULoeyHmdiPchP5DzBrkeuRRzAZpz9Qf8zLoJXweU-rqKEGnguFnFWlb2rUwNF13Ec15JpUQq7BKH56DUqWG-HrG2o =217x79)  Chi-Square of each node is calculated using formula,\nChi-square = ((Actual ‚Äì Expected)^2 / Expected)^1/2\nInformation Gain [Categorical]\nInformation theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% ‚Äì 50%), it has entropy of one. Based on the concept that impure nodes require more information to describe it and vice versa.\n![](https://lh4.googleusercontent.com/rcMfiGKMgvSZzjPnQX33elyNQdYUwaxpTvsWdwPx8iCWLLLtNiSDjd6lxwV8IYPcjnklyeW2cKr-g7l0Ljv-4yTdZv3bSiVnuZc23Usjh9nqk4RwmBTU9sEwCQ488T4rQs5lPYYo =218x30)\nHere p and q is the probability of success and failure respectively in that node. The lesser the entropy, the better it is.\nInformation Gain = 1 - Entropy\n![](https://lh4.googleusercontent.com/zybRY0XxMY_eYoZsmIEtr-uvOt7958kk9xRpU-kQGYAsriJ6wWiGbNlfzjFyJ3ORsw0aB_fLtH2VnS331upispJ9wq6XkIgZP5RbZ_q1i9XBx1Ggm1MkneH6sjj5ct4aP0gF3TAK =131x57)![](https://lh3.googleusercontent.com/wBBoA_r9sduFgt_mtm7xSfnhT0i2G1_RyHZjVl-fvgjj2dMBbd_yS7SG5wuECHk4wBz87nXkesKrLI-TiivQUCum-0C0sIg2II9moYb71Aq5TI06O8L6g5HXi36e88GeKKaKiFV5 =133x65)\n![](https://lh3.googleusercontent.com/R8d8virMuQKr9epf4yd1WkzWkIL8yu3h4K9HJhzrt9YjV43NK988Id0Qwvtsyi9VniiqlqkrgdnJS0hWOYcX2IDkv7Wdcmyg0Bnlwj4yVZRucRGIWH34D-8d_blNCegSKY4Ejud9 =442x50)\n![](https://lh6.googleusercontent.com/u4RjBDBarEnYDWaF25iPUsi3J_sTFJjT9YtezaDcLT0Vi-G9OVO0v9b-sia1rKJd93mkeTseiaedreDQQnKLRu1dToY4k-PWCTUUMpU5NGD_rxwNE52fABjDdJkm_-3KFld8_N65 =318x31)\n![](https://lh4.googleusercontent.com/jRsSIsIBpkX8XQZfggQ0GbXnfHScx2rSyRUx70T_qQPtyNAPWOQIQ_jNLTLUIXN-Uq0f1I-7937R45RBum224v8S2RnhTh1eeiO3sGfPcq_-mUu5ubmVRHOjREXZzoJmKLXg_sm0 =720x60)\nReduction in Variance [Continuous]\n![](https://lh6.googleusercontent.com/Ud0tg7MutTJjtjmZRhtsHy52aF6z78mNlhwvMUL95-dAFZEexKbnbLKXoXatkCtr8oH44wHTve7vPo9t102DdlV9zsM5ucWyWl8KuYA9XaLsB1dhKa_tdDwRgPe_cdFVNLy6FzZb =176x53)\nReduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population\nX-bar is mean of the values, X is actual and n is number of values.\nAlgorithms CART : Classification And Regression Trees Decides on split based on the amount of homogeneity within class that is achieved by the split.\nClassification y is categorical\nRegression y is continuous\n CART first grows the full tree using training set and then prunes it back using holdout set. Allows only binary splits.  Gini algorithm\nID3 \u0026gt; C4.5 \u0026gt; C5.0 : Splitting criterion is the normalized information gain (difference in information entropy). The attribute with the highest normalized information gain is chosen to make the decision. It is the successor of ID3.\n C4.5/5.0 first grows the full tree using training set and then uses single dataset to arrive at final tree. Allows multiple split.  CHAID : Chi-square Automatic Interaction Detector Tests a hypothesis regarding dependence between the split variable and the categorical response (using chi-square test for independence)\n  Similar to regression analysis, it selects best predictors that account for the most explained variable\n  Chaid goes one step further and identifies the important elements : those variables that most differentiate target variable.\n  Chaid begins by finding variables that have significant association with target variable\n  It assesses the category groupings or interval breaks to pick the most significant combination of variables\n  The variable having the strongest association with the target variable becomes the first branch in a tree with lead for each category that is significantly different relative to target variable.\n  The process is repeated to find the predictor variable on each leaf most significantly related to the target variable, until no significant predictors remain.\n  Chaid uses statistical stopping rule that discontinues tree growth. It uses training dataset\n  Allows multiple split.\n  ","description":"Decision Trees","id":5,"section":"handbook","tags":null,"title":"Decision Trees","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/2-decision-trees/"},{"content":"Gradient Boosting (GBM) \u0026amp; AdaBoost The base learner takes all the distributions and assign equal weight or attention to each observation.If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\nweight = ln ( accuracy / (1-accuracy) ) = ln (correctly classified / incorrectly classified)\nThe fundamental difference between Bagging and Random Forest is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n","description":"Gradient Boosting","id":6,"section":"handbook","tags":null,"title":"Gradient Boosting","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/"},{"content":"Measures of Dispersion/Spread    Name Spread     Range Max - Min value   Percentile Divides the data into 100 equal parts using 99 points   Decile Divides the data into 10 equal parts using 9 points   Quartile Divides the data into 4 equal parts using 3 points (25%, 50%, 75%) Median is the 2nd quartile (50%) . Interquartile range : 25% - 75%     Boxplot uses a limit of 1.5 IQR (Inter-Quartile Range) at whiskers to identify outliers\n Standard Deviation (œÉ) How far are the data dispersed from the mean. How much the members of a group differ from the mean value for the group. sq-root of variance.\nœÉ (population) | s (sample) | Unit = same as data\nVariance (œÉ2) Indicates how spread out the data are.\nUnit = data squared\n Total Error = Sum of deviances from the mean = ‚àë (xi - xÃÑ) Sum of squared error (SSE) = ‚àë (xi - xÃÑ)2 Variance = SSE ‚ûó (n-1) for estimating population Variance = SSE ‚ûó (n) for sample  Standard Error (of the mean) (SEM) (using the sample means to estimate the population mean) is the standard deviation of all sample means (of a given size). [see Central Limit Theorem]\nœÉ √∑ ‚àön\nConfidence Interval CI = mean ¬± [(z-scores for confidence level)*standard error]\nZ Scores z is a unit of measure that is equivalent to the number of standard deviations a value is away from the mean value.\nZ = (Data Point - Mean) / Standard Deviation Eg : if z = 1.79 , then it is 1.79 œÉ away from the mean*\n","description":"Measures of Dispersion/Spread","id":7,"section":"handbook","tags":null,"title":"Dispersion/Spread","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/"},{"content":"Types of Regression Simple Linear Regression It is the simples form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)\nMultiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It as one Dependent Variable and Multiple Independent Variable. Hence the name. In multiple regression, the equation would be :\nPolynomial Regression Polynomial regression is another form of regression in which the maximum power of the independent variable is more than 1. In this regression technique, the best fit line is not a straight line instead it is in the form of a curve.\nQuadratic regression, or regression with second order polynomial, is given by the following equation:\nY =Œò1 +Œò2x +Œò3x2\nStepwise regression is by no means a regression. It\u0026rsquo;s a (flawed) method of variable selection.\n","description":"Types of Regression","id":8,"section":"handbook","tags":null,"title":"Types of Regression","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/"},{"content":"Breifly describing how a recommendation engine works\n","description":"Recommendation Engine","id":9,"section":"information","tags":null,"title":"Recommendation Engine","uri":"https://do2blehelix.github.io/the-ml-handbook/information/recommendation-engine/"},{"content":"SVM (Support Vector Machine) ![](https://lh5.googleusercontent.com/n0CPoXbAgg0MNx5jxNNmn14h-aWrPgVDUj4uo_6DqnUL4iRX7ZHTjl8GoDwXn1IWbp1743NgaDXva8rDUtac5oKaPdAZMbJ4qaOqNx23JVCZHwEOwyeLdizmFHJG57oHKNidmboV =308x209)\nIn this algorithm, each data item is plotted as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.\nThe aim is to determine the location of decision boundaries also known as hyperplane that produce the optimal separation of classes. Multiple frontiers are produced to suit the data. The best frontier is the one which is farthest away from the nearest support vector.\nIn case the data cannot be clearly segregated, the data is transformed to a higher dimensional plane where it can be segregated easily.\nClassification Error : Split the data into 2, create boundary lines, classify anything between the margin as error, add errors.\nMargin Error : Goal is to obtain a larger margin between the 2 boundary lines with lower error.\nAdvantages : Robust, accurate classification\nDisadvantages : Computationally expensive\nHyperparameters :\nC-parameter : It\u0026rsquo;s just a constant that attaches itself to the classification error.\nSmall c = large margin and some classification errors. Large c = classify well but small margin.\nKernel :\n Linear Polynomial Kernel :  Degree   RBF (Radial Basis Functions) Kernel :  Œ≥ Parameter : small Œ≥ = wide curve / large Œ≥ = narrow curve    SVMs can be implemented:\n  Maximum Margin Classifier : When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors)\n  Classification with Inseparable Classes :Data in the real world is rarely completely separable. For this reason, we introduced a new hyper-parameter called C. The C hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary. The value of C ranges between 0 and infinity. When C is large, you are forcing your boundary to have fewer errors than when it is a small value.\n  Note: when C is too large for a particular set of data, you might not get convergence at all because your data cannot be separated with the small number of errors allotted with such a large value of C.\n Kernel Methods : Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear. Specifically, you saw two types of kernels: polynomial, rbf. By far the most popular kernel is the rbf kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another hyper-parameter gamma. When gamma is large, the outcome is similar to having a large value of C, that is your algorithm will attempt to classify every point correctly. Alternatively, small values of gamma will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data.  ","description":"Support Vector Machine","id":10,"section":"handbook","tags":null,"title":"Support Vector Machine","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/3-support-vector-machine/"},{"content":"XGBoost XGBoost stands for Xtreme Gradient Boosting. It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Can be used to solve regression, classification, ranking, and user-defined prediction problems. XGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. However, XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.\n","description":"XG Boost","id":11,"section":"handbook","tags":null,"title":"XG Boost","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/3-xgboost/"},{"content":"Distributions Normal / Gaussian / Continuous Distribution data distributed symmetrically around the center skewness = kurtosis = 0. Mean = Median = Mode. Also known as the bell curve.\nBinomial Distribution Discrete distribution used in statistics. Only counts 2 states typically 0 and 1.\nUniform Distribution Consists of similar values throughout\nSkewed Distribution Data distributed which spikes towards either ends as opposed to the central spike in normal distribution (skewness: lack of symmetry)\nKurtosis : pointiness of the curve\nPositive kurtosis = leptokurtic | Negative kurtosis = platykurtic\n   Data dist type Measure of Central Tendency Measure of spread (variation)     Normal Mean Standard Deviation   Non normal (skewed) Median Range, Percentile \u0026amp; IQR     To convert any dataset with any mean \u0026amp; std deviation to a dataset with mean = 0 \u0026amp; std dev = 1.\nCan be done using z-scores: z = (x - xÃÑ) √∑ s\n Central Limit Theorem The distribution of the sample means tends towards a normal distribution as the number of samples increase\n Take a random population and plot its distribution (assuming distribution ‚â† normal distribution) Samples of constant size n are to be taken from the population (n1 = n2 = n3 ‚Ä¶) Take a random sample of size n1 from the population and calculate its mean xÃÑ1 Take a random sample of size n2 from the population and calculate its mean xÃÑ2 Plot the means xÃÑ1, xÃÑ2, xÃÑ3 ‚Ä¶ The sampling distribution (plot of xÃÑ1, xÃÑ2, xÃÑ3 ‚Ä¶) tends to be a normal distribution as the number of samples increases The variance of the sampling distribution = The variance of the population / sample size œÉxÃÑ2 = œÉ2 / n With a higher value of n, the sampling distribution tends to have a lower variance (high kurtosis), and vice-versa The standard deviation of the sampling means is called Standard Error of the Mean.  ","description":"Distributions","id":12,"section":"handbook","tags":null,"title":"Distributions","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/"},{"content":"Regularization Ridge Regression Lasso Regression ","description":"Regularization","id":13,"section":"handbook","tags":null,"title":"Regularization","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/"},{"content":"Na√Øve Bayes It is a classification technique based on Bayes‚Äô theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. P(c|x) is the posterior probability of class (target) given predictor (attribute). P(c) is the prior probability of class. P(x|c) is the likelihood which is the probability of predictor given class. P(x) is the prior probability of predictor.  Calculation   Convert the data set to frequency table\n  Create Likelihood table by finding the probabilities\n  Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n  Naive Bayes predicts the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.\n          Known P(A) Probability of A    Known P(R|A) Probability of R given A    Bayes theorem infers P(A|R) Probability of A given R      This is the \u0026lsquo;Naive\u0026rsquo; bit of the theorem where it considers each feature to be independent of each other which may not always be the case and hence that can affect the final judgement.\n ","description":"Naive Bayes","id":14,"section":"handbook","tags":null,"title":"Naive Bayes","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/4-na%C3%AFve-bayes/"},{"content":"Covariance \u0026amp; Correlation Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people.\n‚ö†Ô∏è Correlation doesn\u0026rsquo;t imply causation\nCovariance : Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance.\nReturns a value between -‚àû \u0026amp; +‚àû\nSum [(X-u1) (Y-u2)] / (n-1)\nCorrelation : Correlation is the standardized form of covariance.\nReturns a value between 0 (very weak) \u0026amp; ¬±1 (very strong), inclusive\nMulticollinearity : describes a linear relationship between two or more variables\nAutocorrelation : describes correlation of a variable with itself in a time lag\nPearson Correlation (Karl Pearson)  Measures the strength of linear dependence between two variables A correlation of 0 doesn‚Äôt imply ‚Äòno relationship‚Äô between the 2 variables, it just implies ‚Äòno linear relationship‚Äô Calculated as the covariance of X and Y divided by product of standard deviation of X and Y Denoted as ùùÜ (rho) for population and r for sample correlation coefficient  r = i=1n[(xi- x) (yi-y)]i=1n(xi- x)i=1n(yi- y)\nSpearman Correlation  Is a non-parametric technique. Does not test a linear relationship but the strength of monotonicity between 2 variables ie the direction. Immune to outliers Can detect non-linear monotonic relationships easily as highly correlated. Divides the variable observations into ranks and runs correlation analysis based on the ranks instead of the original observation data.  However, Spearman correlation values tend to be subdued as compared to Pearson‚Äôs when detecting linear relationships: due to removal of impact of squared distances\n","description":"Covariance \u0026 Correlation","id":15,"section":"handbook","tags":null,"title":"Covariance \u0026 Correlation","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/"},{"content":"Artificial Neural Network (ANN) Artificial neural networks (ANNs) are types of computer architecture inspired by biological neural networks (Nervous systems of the brain) and are used to approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are presented as systems of interconnected ‚Äúneurons‚Äù which can compute values from inputs and are capable of machine learning as well as pattern recognition due their adaptive natureAn artificial neural network operates by creating connections between many different processing elements each corresponding to a single neuron in a biological brain.Implemented on a single computer, an artificial neural network is normally slower than more traditional solutions of algorithms. The ANN‚Äôs parallel nature allows it to be built using multiple processors giving it a great speed advantage at very little development cost.\nThe difference between neural networks and deep learning lies in the depth of the model. Deep learning is a phrase used for complex neural networks.\n**Layers :**Input Layer : accepts inputsHidden Layer : set of linear models created with the first layerOutput Layer : linear models get combined to form a non-linear model\nPerceptron vs Gradient DescentIn GD we take the weights and change them by Wi to Wi + a(y-y)xi [y can be any number between 0 \u0026amp;1]In Perceptron, not every point changes weights, only the misclassified ones. [y` can take 0 or 1 only]\nActivation Function :\nIn order to use 0/1 variables for classification, the gradient descent algorithm needs the error to be continuous and not discrete. In case the values are discrete, an activation function is used.The activation function converts the discrete step function (0/1) to a continuous function.\nSigmoid Function converts the numbers to a 0-1 scale with values closer to 1 for large positive numbers,values closer to 0 for large negative numbers and values close to 0.5 for numbers close to 0Softmax Function is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes (outcomes).The softmax function requires all psigma(x) = 1 / (1 + exp(-x))Hyperbolic Tangent Function : Similar to sigmoid, but range between -1 to 1.Rectified Linear Unit (ReLU) : Simple function. If positive then same value is returned, if negative then returns 0.\nMaximum Likelihood\nCross Entropy: If we have a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities.If it\u0026rsquo;s very likely, then we have a small cross entropy. **Lower cross entropy is better.**If it\u0026rsquo;s unlikely, then we have a large cross entropy.\nSum of the negatives of the logarithms of the probabilities of the points. :: -ln(probability)\nThe likelihood or probability of an event is inversely proportional to the cross entropy.\n**Error Function (Loss Function) :**Gradient Descent Algorithm (log-loss error)The error function should be differentiableThe error function should be continuousMean Squared Error\nFeed Forward :\nEpoch : One epoch is when an entire dataset is passed both forward and backward through the neural network only once\n**Backpropagation :**In a nutshell, backpropagation will consist of:Doing a feedforward operation.Comparing the output of the model with the desired output.Calculating the error.Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.Use this to update the weights, and get a better model.Continue this until we have a model that is good.\nTraining :\nEarly Stopping : Keep doing Gradient Descent until the testing error stops decreasing and starts to increase. At that point we stop.Regularization : Large coefficients = Overfitting // Penalize large weightsL1 : Add sums of the absolute values of the weights, times lambda *( vectors with sparse weights :: good for feature selection)*L2 : Add the sum of the squares of the weights, times lambda. *(vectors with homogeneous weights :: better for training models)*Lambda = constant which tell how much to penalize the coefficientsDropout : Hold out nodes and feed-forward and back-propagate one epoch. Probability that each node will be droppedRandom Restarts : To avoid Local Minimas, which prevent the Gradient Descent from arriving at the bottom (global minimum), the Gradient Descent algorithm is started from a few places randomlyLearning Rate : How but steps are taken in order to reach the global minima. Higher rates are faster but inaccurate and may miss the minima and keep going.Momentum : Is the weight given to each step with the last step given higher weightage. This is done to avoid getting stuck at a local minima by jumping over with the average momentum. B is between 0 to 1.\nL2 is better than L1. Example:PointsL1L2****Inference(0,1) 0 + 1 = 102 + 12 = 1L1 and L2 are both 1 in this case and are similar.(0.5, 0.5)0.5 + 0.5 = 1 0.52 + 0.52 = 0.5L1 is 1; L2 is 0.5. L2 has less error. Hence L2 preferred\nAlgorithm:We are at the top of the error and require a bunch of steps to reduce the error. This is done by Gradient Descent by going down in small steps following the negative of the gradient of the height, which is the error function.Each step is called an epoch.In each epoch, we take out input (data) and run it through the entire neural networkFind predictions and calculate the errorBack propagate the errors in order to update the weights in the neural network.\nIf we have many, many data points, which is normally the case, then these are huge matrix computations, I\u0026rsquo;d use tons and tons of memory and all that just for a single step.If we had to do many steps, you can imagine how this would take a long time and lots of computing power.Is there anything we can do to expedite this? Well, here\u0026rsquo;s a question: do we need to plug in all our data every time we take a step?If the data is well distributed, it\u0026rsquo;s almost like a small subset of it would give us a pretty good idea of what the gradient would be. Maybe it\u0026rsquo;s not the best estimate for the gradient but it\u0026rsquo;s quick, and since we\u0026rsquo;re iterating, it may be a good idea.This is where stochastic gradient descent comes into play. The idea behind stochastic gradient descent is simply that we take small subsets of data, run them through the neural network, calculate the gradient of the error function based on those points and then move one step in that direction.\nActivation function: relu and sigmoidLoss function: categorical_crossentropy, mean_squared_errorOptimizer: rmsprop, adam, ada\nDisadvantagesComputationally intensiveTends to overfit most times.\n","description":"Artificial Neural Network","id":16,"section":"handbook","tags":null,"title":"Artificial Neural Network","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/5-artificial-neural-network/"},{"content":"Hypothesis Testing It the the action of declaring a hypothesis (a fundamental logical component) and proving its occurrence of proving its non-existence.\nTypes of Hypothesis Null Hypothesis  H0 : Œº1 = Œº2x \nThe means of the two groups (Œº1 \u0026amp; Œº2) belong to the same population. (p \u0026gt; 0.05)\n Alternative Hypothesis  HA : Œº1 ‚â† Œº2\nThe means of the two groups (Œº1 \u0026amp; Œº2) belong to different populations. In case of multiple groups, the mean of all groups shouldn‚Äôt be the same. At least one group should be different. (p \u0026lt; 0.05)\n Significance level : Œ± The significance level, also denoted as alpha or Œ±, is the probability of rejecting the null hypothesis when it is true. Typically 0.05 (5%)\nConfidence level : (1 - Œ±) ¬†Errors Type I : Rejecting the null hypothesis even if it\u0026rsquo;s true False Positive\nType II : Accepting the null hypothesis even when it\u0026rsquo;s false False Negative\nProbability (type I error) = Œ± | Probability (type II error) = Œ≤\n¬†Analogy\nPerson on trial :\nNull = He is innocent Alternate = He is guilty\nType I : reject the null when true : he is innocent but rejected : innocent person in jail ‚õî avoid\nType II : accept the null when false : he is guilty but declared innocent : guilty person free\nPerson with cancer :\nNull = Doesn't have cancer Alternate = Has cancer\nType I : reject the null when true : doesn‚Äôt have but detected : person lives\nType II : accept the null when false : has but not detected : person dies ‚õî avoid\n¬†Misclassification Errors (Confusion Matrix)\n   Actuals ‚è¨ Predicted ‚è©       Pred ‚úîÔ∏è Pred ‚ùå   Act ‚ùå FP TN   Act ‚úîÔ∏è TP FN    ","description":"Hypothesis Testing","id":17,"section":"handbook","tags":null,"title":"Hypothesis Testing","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\n Headings The following HTML \u0026lt;h1\u0026gt;‚Äî\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae.\nNote that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n‚Äî Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline¬† Markdown¬† In¬† Table     italics bold strikethrough¬† code    Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item  First Sub-item Second Sub-item    Other Elements ‚Äî abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","description":"","id":18,"section":"blog","tags":["markdown","css","html","themes"],"title":"Markdown Syntax Guide","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/markdown-syntax/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nHugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n YouTube Privacy Enhanced Shortcode   Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  ‚ÄúIn addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.‚Äù\n‚Äî Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019 Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }  ","description":"","id":19,"section":"blog","tags":["shortcodes","privacy"],"title":"Rich Content","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/rich-content/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\ninhospita parcite confusaque translucet patri vestro qui optatis\nlumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus\nsilentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria\ntractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra\ndicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere\nfurit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli\nLelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare\nEchionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert\nausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae\nvulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem\nPropoetides parte.\n","description":"","id":20,"section":"blog","tags":["markdown","text"],"title":"Placeholder Text","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/placeholder-text/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nEmoji can be enabled in a Hugo project in a number of ways.\n The emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site‚Äôs configuration and then you can type emoji shorthand codes directly in content files; e.g.\nüôà üôà üôâ üôâ üôä üôä\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3  .emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }  ","description":"","id":21,"section":"blog","tags":["emoji"],"title":"Emoji Support","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/"},{"content":"Density Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN clusters densely packed points and labels the other points as noise.\nAdvantages:  No specification of no of clusters Flexibility in shapes and sizes of clusters Able to deal with noise and outliers  Disadvantages:  Border points are assigned to whichever clusters find them first Faces difficulties in finding clusters of varying densities.\n(variation of DBSCAN, HDBSCAN can be used to rectify)  Theory: Epsilon (Œµ) = Search Distance around a point\nMinimum number of points = min no of points to be classified as a cluster\nTypes of Points:  Noise point : Fails to meet the minimum no of points criteria Core Point : Meets the minimum no of points criteria Border Point : Is in vicinity of core point and contributing to it, but individually fail to meet the criteria min no of points.  Validation Use DBCV (Density Based Cluster Validation) to score DBSCAN. Other validation metrics will mess up the score.\nOPTICS OPTICS Clustering stands for Ordering Points To Identify Cluster Structure. It draws inspiration from the DBSCAN clustering algorithm. It adds two more terms to the concepts of DBSCAN clustering. They are:-\n Core Distance: It is the minimum value of radius required to classify a given point as a core point. If the given point is not a Core point, then it‚Äôs Core Distance is undefined. Reachability Distance: It is defined with respect to another data point q(Let). The Reachability distance between a point p and q is the maximum of the Core Distance of p and the Euclidean Distance(or some other distance metric) between p and q. Note that The Reachability Distance is not defined if q is not a Core point.  OPTICS Clustering v/s DBSCAN Clustering:  Memory Cost : The OPTICS clustering technique requires more memory as it maintains a priority queue (Min Heap) to determine the next data point which is closest to the point currently being processed in terms of Reachability Distance. It also requires more computational power because the nearest neighbour queries are more complicated than radius queries in DBSCAN. Fewer Parameters : The OPTICS clustering technique does not need to maintain the epsilon parameter and is only given in the above pseudo-code to reduce the time taken. This leads to the reduction of the analytical process of parameter tuning. This technique does not segregate the given data into clusters. It merely produces a Reachability distance plot and it is upon the interpretation of the programmer to cluster the points accordingly.  ","description":"","id":22,"section":"handbook","tags":null,"title":"DBSCAN","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/"},{"content":"Gaussian Mixture Model Clustering Each point belongs to every cluster, but has a different level of membership. It assumes that each cluster has a certain statistical distribution.\nExpectation - Maximization Algorithm\nAdvantages :  Soft Clustering, sample members of multiple clusters. Cluster shape flexibility. (cluster can contain another cluster inside of it)  Disadvantages :  Sensitive to initialization values Slow convergence rate Possible to converge to a local optimum  Process:  Initialize k Gaussian distributions Soft cluster the data into the Gaussian distributions (Expectation step) Re-estimate the Gaussian (Maximization step) Evaluate log-likelihood to check for convergence Repeat from Step.2 until convergence  ","description":"","id":23,"section":"handbook","tags":null,"title":"Gaussian Mixture Model","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/"},{"content":"Hierarchical Clustering It is a set of nested clusters organized as a hierarchical tree. No decision about number of clusters It is not used when data is big due to higher processing time.\nTypes:\n Agglomerative : Start from n clusters and get to one cluster Bottom up approach Divisive : Start from one cluster and get to n clusters Top down approach  Advantages:  Produces an additional ability to visualize Potent esp if the data contains real hierarchical relationships (eg evolution)  Disadvantages:  Computationally intensive Sensitive to noise and outliers  Distance Between Clusters (Agglomerative Clustering)  Single link : Shortest distance between an element in one cluster and an element in another cluster. Complete link : Largest distance between elements in two clusters. Produces compact clusters. Average link : Average distance of elements between 2 clusters. Centroid : Distance between the centroids of 2 clusters Metroid : Distance between centrally located object in both clusters. Ward‚Äôs method : Minimize variance between 2 clusters.  Visualization: ![](https://lh5.googleusercontent.com/jC8mI4S66hPh4i3tr-QZgKZDkkZPrhh0sGcczx2x1BVWLgA0lfXTAH5ncDKa1HkHelh5tfwBASLD3puoEnFb_iOA3LxYpm7MlV8Lqsd-F2BElXVDV0HEVMZEB84QjCRjGdpOS2YS =326x205)\nThe results of hierarchical clustering can be shown using dendrogram.\n At the bottom, we start with n data points (observations), each assigned to separate clusters. Two closest clusters are then merged till we have just one cluster at the top The height in the dendrogram at which two clusters are merged represents the distance between two clusters in the data space.  The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.\n","description":"","id":24,"section":"handbook","tags":null,"title":"Hierarchical Clustering","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/"},{"content":"K means [Non Hierarchical] It is based on division of objects into non overlapping subsets. Main objective is to form clusters that are homogeneous in nature and heterogeneous to each other.\n‚ùï Only for continuous variables.\nAdvantages:  Faster, more reliable, works with large data. Computationally lighter than other methods  Disadvantages:  Can only identify clusters circular / spherical in nature. (check crescent dataset) Distance based  Process:  Identify value of ‚Äòk‚Äô Assign random k observations as seeds Assign each record to one of the k seeds based on proximity Form clusters Calculate centroids of clusters Assign centroids as new seed Form new clusters Recalculate clusters Continue process until stable clusters are formed (boundary ceases to change)  Elbow Criterion (Scree Plot):![](https://lh5.googleusercontent.com/VgUz4jopV1BT6doFeT_UOv2Iao0kbY6Ij6ErVBweUUjoQcTSfdA1AbwNcAToMZRo3yZgcEnMtrrDPY6UzniG5Oec_-otvyy7_w7SmeSpKy3AnxH3NHQq4U90uftzY254_OCS5fZr =247x126) K means clustering doesn\u0026rsquo;t provide an estimate of the number of clusters required. Hence elbow criterion is used to determine optimal number of clusters.\nThe method states that you should choose a number of clusters so that adding another cluster does not add any sufficient information. It is plotted by ratio of within cluster variance to between cluster variance against number of clusters. The objective is to minimize the within and maximize the between distances.\nValidation:  Silhouette Index Davies Bouldin Score Calinski Harabasz Score Pseudo F  Implementation: from sklearn.cluster import KMeans km = Kmeans(n_clusters=2, max_iter=100) km.fit(X_std) ","description":"","id":25,"section":"handbook","tags":null,"title":"K-Means","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/"},{"content":"KNN (K- Nearest Neighbors) It is a simple algorithm which classifies cases based on its votes by its nearest neighbors whose class is already known. The ‚ÄúK‚Äù is KNN algorithm is the number of nearest neighbors we wish to take vote from. The class to be chosen depends on a distance function.\nCategorical : Euclidean, Manhattan, Minkowski\nContinuous : Hamming\nAdvantages:  Attributes with multiple missing values can be easily treated Can predict both qualitative \u0026amp; quantitative (by taking average) attributes Easy interpretation. Low calculation time.  ","description":"","id":26,"section":"handbook","tags":null,"title":"KNN","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/"},{"content":"Natural Language Processing NLP can be divided in supervised and unsupervised techniques\n Supervised:  Text Classification (Spam Recognition, labeling, etc) Spam Detection Sentiment Analysis Intent Classification Multi-Label, Multi-Class Text Classification   Unsupervised: Topic Modeling  Applications\n Sentiment Analysis Speech Recognition Chatbot Machine Translation (Google Translate) Spell Checking Keyword Search Information Extraction Advertisement Matching  NLU - Natural Language Understanding  Mapping input to useful representations Analyzing different aspects of the language  NLG - Natural Language Generation  Text planning Sentence planning Text realization  Ambiguities  Lexical Ambiguity - Two or more possible meanings in a word.  She is looking for a match (matchstick vs partner) The fisherman went to the bank (riverbank vs bank)   Syntactic Ambiguity (aka structural/grammatical ambiguity) - Two or more possible meanings in a sentence or a sequence of words.  The chicken is ready to eat   Referential Ambiguity - Referring to something using pronouns  The boy told his father about the theft. He was very upset.    (Processing) Terminologies Step1: Tokenization Process of breaking the string into tokens, which are small structures eg words and special characters.\n Break a complex sentence into words Understand the importance of each word Produce a structural description on an input sentence.  Bigrams, Trigrams, and Ngram - Token of 2,3 or n number of words written together\nfrom nltk.tokenize import word_tokenize \u0026lt;\u0026gt; = word_tokenize(\u0026lt;\u0026gt;) # check distribution for word in \u0026lt;\u0026gt; fdist=[word.lower()]+=1 fdist from nltk.util import bigrams, trigrams, ngrams singles = nltk.word_tokenize(\u0026lt;string\u0026gt;) doubles = list(nltk.bigrams(\u0026lt;string\u0026gt;)) quads = list(nltk.ngrams(\u0026lt;string\u0026gt; , 4))  Step2: Stemming Normalize words into its base or root form by cutting prefixes or suffixes. Eg affects, affected, affecting, affection are stems of affect.\nCommon Stemmers: Porter Stemmer (lenient) ; Lancaster Stemmer (aggressive) ; Snowball Stemmer (requires language input)\nfrom nltk.stem import PorterStemmer from nltk.stem import LancasterStemmer PorterStemmer.stem(\u0026lt;word\u0026gt;) LancasterStemmer.stem(\u0026lt;word\u0026gt;) #aggressive stemmer  Lemmatization Considers the morphological analysis of the word. Needs a detailed dictionary to link the word back to its lemma.\n Groups together different inflected forms of the word, called Lemma. Somehow similar to stemming as it maps several words to a common root. Output is a proper word  Eg gone going and went all maps to go\nfrom nltk.stem import wordnet from nltk.stem import WordNetLemmatizer WordNetLemmatizer.lemmatize('\u0026lt;word\u0026gt;')  Stop Words Sentence forming words not required for language processing. (eg: I, me, we, our, he, him_)_\nParts of Speech (POS) Breaking the sentence into different parts of speech (eg, Noun, verb, adverb, adjective, etc.)\nNamed Entity Recognition (NER) Connects the word to a named entity (eg: Movie, Organization, Person, Location, etc.). Knowledge Graphs are used to\nSyntax Set of rules, principle and process that govern sentence formation\nSyntax Tree: representation of syntactic structure of sentences or strings\nChunking Picking up Individual pieces of information (words, tokens) and grouping them into bigger pieces.\nTopic Modeling Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. It is an unsupervised ML technique where the model is not trained from before.\nAdvantages: Quick, easy start\nDisadvantages: Since unsupervised, lesser accuracy.\nTopic Classification: Topic modeling used with classification (supervised learning). Tags are needed to be created to predefine classes.\nMethods:\n Latent Semantic Analysis (LSA) Latent Dirichlet Allocation (LDA)  Sentiment Analysis ","description":"","id":27,"section":"handbook","tags":null,"title":"NLP","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/nlp/"},{"content":"BIRCH ","description":"","id":28,"section":"handbook","tags":null,"title":"Others","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/"},{"content":"Spectral Clustering It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.\n","description":"","id":29,"section":"handbook","tags":null,"title":"Spectral Clustering","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/"},{"content":"Statistical Tests Statistical tests are conducted to accept or reject hypothesis. They can be divided into tests catering to specific distributions of data.\n Parametric Test : used for normally distributed data (assess group means) Non-Parametric Test : used for skewed data (assess group medians)  The tests are also classified according to their tails.\n One Tailed : uni-directional (typing speed increases with more typing)  Two tailed: bi-directional (typing speed can increase or decrease with more typing)  ‚ö† P-value is the probability of the sample means coming up equal to or even further away from the hypothesized population mean.\nz-Test (sample size \u0026gt; 30) Statistical calculation used to compare sample mean to population mean.\nMost useful when standard deviation and the sample size is known.\n The z score tells how far, in standard deviations, a data-point is from the mean.\n t-Test (all sample size also referred as sample size \u0026lt; 30 ) To determine if there is a statistically significant difference between two sample group means.\nUsed when population standard deviation is unknown.\n The t-score is a ratio between the difference between the two groups and the difference within the groups\n (A large t-score tells you that the groups are different. A small t-score tells you that the groups are similar)\n- Paired samples t-Test is used when there is a before - after ideology (matched)\n- Independent samples t-Test (Equal Variances/Unequal Variances) is also known as a regular t-Test\nANOVA It is basically an extension to t test used when more than 2 samples are to be compared.\nIt is used to determine whether there are any statistically significant differences between the means of three or more independent groups.\nANOVA uses categorical independent variables and a continuous dependent variable\nH0 : all the means of the groups are NOT statistically different (All samples are from the same population)\nHA : at least two group means are statistically different from each other. (At least one sample comes from a different population)\nF = (Between Group Variability) √∑ (Within Group Variability)\nChi-square To test dependency or independency of categorical variables. Are 2 categorical variables independent\nAnuj says it checks the difference in means + variance\nObserved vs expected\nF test It\u0026rsquo;s a significance of variance test used for model evaluation. It is a one way ANOVA.\nIt is basically the ratio of 2 chi square tests.\n¬†| When to use what test | Cheatsheet Python tests |\n","description":"","id":30,"section":"handbook","tags":null,"title":"Statistical Tests","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/"},{"content":"Singular Value Decomposition ","description":"","id":31,"section":"handbook","tags":null,"title":"SVD","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/"},{"content":"Time Series (Forecasting) [Method = Box-Jenkins (B-J)] Components Trend : A long term (relatively smooth) pattern that usually persists for more than one year\nEg : increasing no of flight passengers\nSeasonal : Pattern that occurs at regular intervals. [multiple times in a year (least is once a year)]\nEg December sale\nCyclical : Pattern that occurs over a long time, generally continues over a year\nEg : Recession\nRandom : The component obtained after the above patterns have been extracted.\nModels White Noise : Series is purely random in nature (best take average of model)\nAutoRegressive Model : (p) : Past Terms\nNo of past values to be taken into account = parameters = no of beta coefficients taken into equation.\nYt = f ( Yt-1 , Yt-2 , Yt-3 , ‚Ä¶.. Œµ )\nMoving Average Model : (q) : Error Terms\nForecast by taking the error terms. The error terms are assumed to be white noise with mean zero and a constant variance.\nYt = f ( Œµt-1 , Œµt-2 , Œµt-3 , ‚Ä¶.. )\nARMA Model : (p,q)\nCombines AutoRegressive and Moving Average models\nY = [ B0 + B1Yt-1 + B2Yt-2 + B3Yt-3 + Œµt ] + [ œï0 + œï1Œµt-1 , œï2Œµt-2 , œï3Œµt-3 , ‚Ä¶.. Œµt-q ]\nARIMA Model : (p,d,q)\nCombines ARMA with differencing (I)\nI = 1 : yt - yt-1 // I = 2 : (yt - yt-1) - (yt-1 - yt-2) eg: 25, 16, 9, 4 : (25-16) - (16-9) : 9, 7\nARIMA (1,0,0) = AR Model ARIMA (0,0,1) = MA Model ARIMA (1,0,1) = ARMA Model ARIMA (0,1,0) = Random walk Model\nPositive auto-correlation to higher lags -\u0026gt; higher order differencing\nZero or negative for first order -\u0026gt; no differencing needed\nOptimal order of differencing -\u0026gt; best prediction\nExponential Smoothing Model :\nTakes the average of the last few observations\nStationarity![](https://lh4.googleusercontent.com/P7qQPE2R56lbKYrZQJqKjA-odnrowOTjhbLmasBIUHKvQy5-fyUB1euJXIFp4z8tiRfa5gNOlnoCqFjuB4QNUcZPuWsUju2VuLGF0MjzzUcbnXDn4BMbv6_dX9IhrOzBmVfjOGJx =101.19249841068017x40) Stationary Series : In a stationary series the mean variance and autocorrelation are all constant over time.![](https://lh4.googleusercontent.com/UqUWgR9gRptIANMGTm3Zx9Kh8LPd7wO5lwgENgLywi-AxiC8chLa8fPh4OC2yPr5kwrF1KQVELq8PW3pp2knRuSgqNWVnd_pWFE4QBw2oWwRlNgoMbdeKDBasFoqM3-CIMWMfZNY =150.60944206008583x55)![](https://lh3.googleusercontent.com/gDfimTayxUIEfOGgH6EZNn_G4n5p7lgJ7a6wkkMED_4Ozb4nRVKQhWt2yFyNwt1wZC3R-vYPt_F2x4adCk8Z28I5gcbi7UAA7xnsUCAXEQ_JBMwHlIRuBeNmS1zPbE35kSliXWbb =100.7469670710572x42)\n The mean of the series should not be a function of time. It should be a constant The variance of the series should not be a function of time The covariance of the i th term and the (i + m) th term should not be a function of time![](https://lh5.googleusercontent.com/KPxuhEbavlyk2wl16xpM-l0GLi_qf6RY73791jSJTL3Oa5H7GUtVCFEk4dxJ4UW0slKGFUIWuSkykKCXKshorir50xORUvlramcLNViXOQZEi1v-3X9eWqBOe6Le8sHbnvrmwFu7 =112.65901060070674x42)  Types of Stationarity : Strictly Stationary / Weakly Stationary / Covariance Stationary\nA series which is non stationary can be made stationary after differencing (called as Integrated) hence ARIMA is used for non stationary while ARMA for stationary variables.\nDickey Fuller Test of stationarity : if null hypothesis is rejected then we get a stationary time series.\n![](https://docs.google.com/drawings/u/0/d/sck4Fucjt68q1SJCOeBRq7Q/image?w=222\u0026amp;h=201\u0026amp;rev=278\u0026amp;ac=1\u0026amp;parent=1DFQeQ9FRGL5QV2y-d85pI0R563x5zlvv_Ln5yTr6x-w =222x201)\nThe estimation and forecasting of univariate time series is carried out by Box-Jenkins (B-J) methodology. It is only applicable to stationary variables.\n Identification  AutoCorrelation Function (ACF) : It measures a simple correlation between current observation (Yt) and the observation (p) periods from the current one (Yt-p) eg Y5 ~ Y4 Partial AutoCorrelation Function (PACF) : Used to measure the degree of association between Yt and Yt-p with the effects at intermediate time lags removed. eg Y5 ~ Y1 Inference from ACF and PACF : To see to what extent the current values of a series are related to past values.    Correlograms are plotted (plot of ACF vs lags) to find the initial values for the order of p \u0026amp; q of the AR and MA process. This is done by looking at the ACF and PACF coefficients and finding the pattern from the table\n Estimation\n Yule Walker Procedure Method of Moments Maximum Likelihood Method    Diagnostic Checking : Different methods can be obtained for various combinations of AR and MA\n Lowest value of AIC / BIC / SBIC Plot of residual ACF : should be random / no pattern in error / mean should be zero / residuals should be white noise    Check if data is regularly spaced and same intervals\n  Format dates appropriately\n  Sort the dates\n  Check for missing values\n proc expand -\u0026gt; interpolate missing values (can also aggregate data)    Original Time Series - Seasonal Component = Seasonally Adjusted\n","description":"","id":32,"section":"handbook","tags":null,"title":"Time Series / Forecasting","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/"},{"content":"Cluster Validation Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.\nCompactness : how close the data are to each other (within-cluster-variance)\nSeparability : how far/distinct clusters are from each other (between-cluster-variance)\n Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)\n Internal Indices: Internal Indices measure how good the clusters are in terms of within-cluster-variance and between-cluster-variance. Some of the measures are :\n‚ö™ Silhouette Index [-1 to 1] The silhouette index measures the average distance between the clusters. The silhouette index displays a measure of how close each point in one cluster is to points in the neighboring clusters.\n**Range :\n**1 = Great Score\n0 = Equal distance from both clusters (overlapping clusters)\n-1 = Misclassified, placed between clusters\nSilhouette Coefficient Si = (b - a) / max(a,b) S = average (S1 , S2 , S3 ‚Ä¶‚Ä¶) a = average distance to other samples in the same cluster b = average distance to samples in the closest neighbouring cluster  ‚ö™ Calinski-Harabasz ‚ö™ BIC ‚ö™ Dunn Index Don\u0026rsquo;t use on DBSCAN as noise will lower scores. Also doesn\u0026rsquo;t work well with rings/circular types of points. Works well with compact clusters which are far away from each other.\nExternal Indices: They require already applied labels to validate against to find out if the clusters had meaning from already labelled data eg Sales)\n Adjusted Rand Score [-1 to 1] Fawlks and Mallows [0 to 1] NMI Measure [0 to 1] Jaccard [0 to 1] F-measure [0 to 1] Purity [0 to 1]  Rand Index (Rand Score) = (a + b) / (n)\na = no of pairs same in both labelled and predicted cluster\nb = no of pairs different in either labelled or predicted cluster\nn = no of points\nRelative Indices bouldin and calinksi and silhoutte ???\n","description":"","id":33,"section":"handbook","tags":null,"title":"‚òëÔ∏è Cluster Validation","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/cluster-validation/"},{"content":"Model Evaluation / Selection / Fits / Validation Train Test Split  Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs.  The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.\nRegression Metrics:  R Square: % of variance in Y that is explained by X. It is defined as the square of correlation between Predicted and Actual values.  ![](https://lh3.googleusercontent.com/q7F8E2RPDOT-odNpAUtV-NLMIEC8cIOUID53ZX_COvkIPu8gvdTy6EG-g9qXPPSp-q1jklJ8BDWBnDd1xlVoyOH_8Szch_MLG-uyub_K69ioQevL9J_QZr4P0qO_PCvtc8lxXezR =153x52)\nR2= SSEIndependent VarSSEIndependent Var + SSEErrors\n Adjusted R Square: Similar to R2. It penalizes for adding impurity (insignificant variables) to the model MSE (Mean Squared Error) : RMSE (Root Mean Square Error) : It measures standard deviation of the residuals.  Model with the least RMSE is the best model\n= sqrt (Sum of Squared Errors) / no of obs = sqrt (mean ( (Actual - Predicted)2 ))\n![](https://lh3.googleusercontent.com/BMkjNWVQCJSEuhhwB7OcrweuDf43cblmp2yL2uqnKb3PeS0U927ylIohcxuWbq9CcIN_6th0vNw38KW8hpQV1nirzTuvho95ri6DFqBfrdDe1WPEXdidt38UEuuvPBfG9Km0Lcz_ =213x59)\nMean Square : Sum of squares / df\n MAE (Mean Absolute Error) : sum( |Error| ) / n Error = Actual - Predicted |Error|=Absolute Error MAPE (Mean Absolute Percentage Error) : { absolute (average [ (Actual - Predicted) / Actual ])} should not exceed ~ 8% - 10% AIC BIC  Loss Functions: objective is to minimise these\n MAE : Mean Absolute Error (mean of the absolute errors) MSE : Mean Squared Error (mean of the squared errors) RMSE : Root Mean Squared Error (square root of the mean of squared errors)  Classification Metrics :  Precision (aka PPV) : TP √∑ (TP + FP) Recall (aka Sensitivity) : TP √∑ (TP + FN) F1 Score : 2x (Precision*Recall) √∑ (Precision+Recall)  Confusion Matrix ![](https://lh6.googleusercontent.com/V1bR60sTazQxuSSGuoaafLFL2gG53aT3Xe-b37gtsjadm_gIlgoXGGMX5zDGBpVNBOoc9jpDwpbzpVwTwiYe8R5FZZoDc9JZ3BMfgQyB_rpTb3yV0W6Y50-gE7RhqUEUQe2zAbyx =292x219) | | Actual (+) | Actual (-) |\n| Predicted (+) | TP | TP |\n| Predicted (-) | FN | TN |\nA confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data\nSensitivity : (True Positive Rate or Recall)\n% of actual +ve predicted as +ve = TP √∑ (TP + FN)\n*Specificity :\n*% of actual -ve predicted as-ve = TN √∑ (FP + TN)\nPositive Predicted Value = TP √∑ (TP + FP)\nNegative Predicted Value = TN √∑ (TN + FN)\nAccuracy = (TP + FP) √∑ (TP + FP + TN + FN)\nMisclassification Rate = (FP+FN) √∑ (TP + FP + TN + FN) = (1-Accuracy)\nBoth sensitivity and specificity should be high for a good model.\n [Best Analogy : identifying a single terrorist in a crowd and sniping him vs bombing the place\nSniping : high sensitivity ; high specificity || Bombing : high sensitivity ; low specificity]\n ![](https://lh4.googleusercontent.com/FkafGOErm0s0Xoe-A6Jz4sPwzuhkiJG5R78igy7yyF75B3dGphFuDXuCV49deEHcmBsXys33YWMcEXvaS3yt2MdGQI8K87-TMtjSUWLIJrjl0WNJgveMCP_bYtQBwk0kxKpVA1SC =234x58)\n| \u0026mdash; | \u0026mdash; | | TP | FN | | FP | TN |\n![](https://lh6.googleusercontent.com/-88NYrtKtm6jDoSaPQOuolCDT-TUMZ9JAJ2In_J5oe8qPpNl5vVH59fCvQrj2YCIqDSbXzi-Us04VI7m9gCNml_ArzF4biKv5fnRjWmutvTY2nxWcgjBdo0ILlDSQIWnmEeNaCaB =209x68)\nMedical Model : High Recall Model :: FN should be avoided\nSpam Model : High Precision Model :: FP should be avoided\nBeta Score FŒ≤ (Range: 0 to ‚àû)\n(FŒ≤=0) Precision ‚Ä¶‚Ä¶ F0.5 ‚Ä¶‚Ä¶. F1 ‚Ä¶‚Ä¶.. F2 ‚Ä¶‚Ä¶. Recall (FŒ≤=‚àû)\nFor other values of Œ≤, if they are close to 0, we get something close to precision, if they are large numbers, then we get something close to recall, and if Œ≤=1, then we get the harmonic mean of precision and recall.\nFor the spaceship model, we can\u0026rsquo;t really afford any malfunctioning parts, and it\u0026rsquo;s ok if we overcheck some of the parts that are working well. Therefore, this is a high recall model, so we associate it with beta = 2.\nFor the notifications model, since it\u0026rsquo;s free to send them, we won\u0026rsquo;t get harmed too much if we send them to more people than we need to. But we also shouldn\u0026rsquo;t overdo it, since it will annoy the users. We also would like to find as many interested users as we can. Thus, this is a model which should have a decent precision and a decent recall. Beta = 1 should work here.\nFor the Promotional Material model, since it costs us to send the material, we really don\u0026rsquo;t want to send it to many people that won\u0026rsquo;t be interested. Thus, this is a high precision model. Thus, beta = 0.5 will work here.\nROC (Receiver Operating Characteristics) Curve![](https://lh6.googleusercontent.com/R_1QXnZi0zNM6jWBEZNXpxStbyq-3GVzOCmODRvJSHYNXfPDmEGUgEUsUWFenRl3lELiF4-VZ55Q924V9p0G9dvJzmmw9TbQ8iVJBAxp3IwuhYSbEh8Dr5ZUdrg7q-Wf1QkBvPj3 =302x189) The ROC chart is plotted with :\nY axis as sensitivity [True Positive Rate] and\nX axis as (1-specificity) [False Positive Rate]\nThe diagonal line represents a random chance model whereas the line curved towards top left represents how better the model is. The more distant the curve, from the random chance line, the better is the model.\nAUC : The area under the curve is also used as a quality measure. A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1\nc = (%concordant - %discordant + %tied) √∑ no of pairs\n(random chance) 0.5 \u0026lt; c-stat \u0026lt; 1 (perfect model)\nK-S Chart![](https://lh5.googleusercontent.com/tvo-uQPoquKBLLN-vNNiApTart8yLaDVtNskblYlqjZcE1dc2qIAEEWSbPuQ9RFnAVKAJRrdW96ENQe9vdybKCnXneAm0TNYar9bzs4QGApe-wjqK6sD79J63vdlE8eBSdvJXCgZ =297x174) K-S is a measure of the degree of separation between the positive and negative distributions (events vs non events). It is defined as the maximum difference between cumulative % event and cumulative % non event.\nThe K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. If the model selects cases randomly from the population, the K-S would be 0.\nKS Statistic is the measure of maximum separability.\n(random chance) 0 \u0026lt; K-S \u0026lt; 100 (perfect model)\nEXAMPLE\nGain and Lift Charts Gain is defined as the cumulative % of target (the ratio of cumulative number of targets (events), to the total number of targets (events))\nLift measures how much better one can expect to do with the predictive model comparing without a model. It is the ratio of gain % to the random expectation % at a given decile level. The random expectation at the xth decile is x%.\n Score (predicted probability) the validation sample using the response model under consideration. Rank the scored file, in descending order by estimated probability Split the ranked file into 10 sections (deciles)  Rank Ordering Same deciles table is created and % of Events is checked to see if there is any discrepancy in downward trend. If the trend is broken then the rank ordering is not maintained.\nConcordance![](https://lh6.googleusercontent.com/q5z3xrwoK98656UAitBaFqZ1qLWa4G_8GLXt5HfLfHmwykQ33uKfpnVA0QzvcRxiv2rwbFaW7ojeVw1jm5-a-KCo_2ob2HNgX4XySmfGFRBDcI5xlBa8mD8e_bZ-bq-FUKy-zQP8 =151x105)  Concordant Pair Discordant Pair Tied Pair  Percent Concordant = (Number of concordant pairs) / Total number of pairs\nPercent Discordance = (Number of discordant pairs) / Total number of pairs\nPercent Tied = (Number of tied pairs) / Total number of pairs\nArea under curve (c statistics) = Percent Concordant + (0.5 * Percent Tied)\nHosmer and Lemeshow Goodness of Fit It is carried out to check if the regression explains the variance in data. It sorts the predicted values and groups them into 10 deciles and compares it against the sorted and grouped dependent variable in the original dataset and finds out if any significant difference between observed and expected.\nChi sq test for difference between observed and expected. High p-value indicates good fit.\nH0 : The data are consistent with a specified distribution.\nHA : The data are not consistent with a specified distribution.\nAIC [Akaike Information Criterion] By adding more parameters, better fit is obtained. However, after a certain point, model tends to overfit.\nAIC captures tradeoff between number of parameters added and the incremental amount of error.\nBasically, it penalizes models for overfitting of data. Also known as loss criterion.\nModel with lower AIC is better !\nAIC = - ln L + p L = likelihood p = parameters\nAIC = N ln (SSerror √∑ N) + 2K N = no of obs K = no of parameters fit+1\nBIC [Bayesian Information Criterion] BIC penalizes the number of parameters more than AIC. Emphasizes simplicity of model.\nGini (Somers‚Äô D) Determines the strength and direction of relationships between pairs of variables.\nIt ranges from -1 (all pairs disagree) to +1 (all pairs agree). Should be gt 0.4\nSomer\u0026rsquo;s D = 2 AUC - 1 = (%concordant - %discordant) / 100\n= (Concordant pairs - Discordant pairs ) / Total Pairs\nGamma Similar to Somers‚Äô D but does not penalize for tied pairs.\nBias - Variance Tradeoff\nBias : how much on average is my predicted values different from actual values. High bias is underfitting.\nVariance : how different will predictions be, at the same point, if different samples are taken from the same population. High variance causes overfitting.\n![](https://lh5.googleusercontent.com/QNFSL6we1fCDjTMHUUqz-WnKg_JuldfEulTu2qvDX7ZoDgoSxFAVhejDThCMLTRzfDqRs7Xll9ECL6NU7x6joEWnY0KXJsEePFabL5cH9pq0k6ya3Lxfyt9iguICnf7PfdXxlX2F =286x170)\nhigh bias = high (pred - act) = high error = underfit model\nhigh variance = high sensitivity in changes in data = overfit model\nWhen a model has high bias, this means that it doesn\u0026rsquo;t do a good job of bending to the data\nWhen a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset\nHigh Bias, Low Variance models tend to underfit data, as they are not flexible. Linear models fall into this category of models.\nHigh Variance, Low Bias models tend to overfit data, as they are too flexible. Decision trees fall into this category of models.\n![](https://lh4.googleusercontent.com/ga9WJ0VSA54deurET54OU6jSsyfQcOFCfABQlXpqVonjll7dvCCz_17NvBrfxrAYS1uXR4XYxDkWoeDkxsYgvirMNjbATe8b1Yjvm2YfhCZ-ZzmSQFAKLSEvv0j6KSjD7XfjqQN- =492x246)\nModel Complexity Graph : Plots the training and testing error and helps find the optimal point between underfitting and overfitting.\n   Validation : K-fold cross Validation  Sample is partitioned into k equal sized subsamples. A single subsample is used as the validation dataset for testing the model. Remaining k-1 subsamples are used as training data. The cross validation process is repeated k times.  Advantages\nAll observations are used for both training and validation.\nEach of the k subsamples used exactly once as the validation data.\nLearning Curves ![](https://lh5.googleusercontent.com/zHfafLEPe9jlrCSeRDQFivuKWDhpNAMsltF8Ulh0-bRd0ANAXuWsrsSn5lyUJI30q2l7Q_sx3-1Qae7Xgr3MmSf53yi7tAPxnCW4oRmrXvxWRgG_F3yooM3f19eYF4DzT1u0SUqZ =427x181)\nGrid Search Technique used to list down all the possibilities of the hyperparameters and pick the best one.\nMODEL MONITORING\nPopulation Stability Index (PSI)\n Sort scoring variable on descending order in scoring sample Split the data into 10 or 20 groups (deciling) Calculate % of records in each group based on scoring sample Calculate % of records in each group based on training sample Calculate difference between Step 3 and Step 4 Take Natural Log of (Step3 / Step4) Multiply Step5 and Step6  continue model \u0026lt; 0.1 \u0026lt; slight change \u0026lt; 0.2 \u0026lt; significant change\n","description":"","id":34,"section":"handbook","tags":null,"title":"‚òëÔ∏è Model Evaluation","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/model-evaluation/"}]