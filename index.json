[{"content":"Markdown here\n","description":"test post","id":0,"section":"updates","tags":null,"title":"May 2019","uri":"https://do2blehelix.github.io/the-ml-handbook/updates/2019_may/"},{"content":"Markdown here\n","description":"test post","id":1,"section":"updates","tags":null,"title":"April 2019","uri":"https://do2blehelix.github.io/the-ml-handbook/updates/2019_april/"},{"content":"Markdown here\n","description":"test post","id":2,"section":"updates","tags":null,"title":"March 2019","uri":"https://do2blehelix.github.io/the-ml-handbook/updates/2019_march/"},{"content":"Markdown here\n","description":"test post","id":3,"section":"updates","tags":null,"title":"February 2019","uri":"https://do2blehelix.github.io/the-ml-handbook/updates/2019_february/"},{"content":"Markdown here\n","description":"test post","id":4,"section":"updates","tags":null,"title":"January 2019","uri":"https://do2blehelix.github.io/the-ml-handbook/updates/2019_january/"},{"content":"Measures of Central Tendency Mean The sum total of units divided by the number of units. Average\nμ (population) | x̄ (sample) | 1/ni=1nxi\nMedian The middle / midpoint value in a sorted sequence. Middle\nIt divides the data 50(more):50(less).\nMode The most commonly occurring value Frequent\nTwo modes = Bimodal ; Multiple modes = Multimodal\n  Q) Why do we use mean most times ?\nA) It takes all values into consideration. Mode/median ignores\n  Q) When do we use median ?\nA) It is less prone to outliers as it doesn’t get affected by them.\n  Nomenclature\n   Population Sample     X = set of population elements x = set of sample elements   N = population size n = sample size   μ = population mean x̄ = sample mean   σ = population std dev s = sample std dev     Normal Distribution Data\n   Percentage μ ± s.d.     68.3 % μ ± 1σ   95.5 % μ ± 2σ   99.7 % μ ± 3σ     Population - our entire group of interest\nParameter - numeric summary about a population\nSample - subset of the population\nStatistic - numeric summary about a sample\n","description":"Central Tendency","id":5,"section":"handbook","tags":null,"title":"Central Tendency","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/"},{"content":"Measures of Dispersion/Spread    Name Spread     Range Max - Min value   Percentile Divides the data into 100 equal parts using 99 points   Decile Divides the data into 10 equal parts using 9 points   Quartile Divides the data into 4 equal parts using 3 points (25%, 50%, 75%) Median is the 2nd quartile (50%) . Interquartile range : 25% - 75%    ℹ Boxplot uses a limit of 1.5 IQR (Inter-Quartile Range) at whiskers to identify outliers  Standard Deviation σ  `σ` (population) | `s` (sample) | Unit = same as data How far are the data dispersed from the mean. How much the members of a group differ from the mean value for the group. sq-root of variance.\nVariance σ2 Indicates how spread out the data are.\nUnit = data squared  Total Error = Sum of deviances from the mean = ∑ (xi - x̄) Sum of squared error (SSE) = ∑ (xi - x̄)2 Variance = SSE ➗ (n-1) for estimating population Variance = SSE ➗ (n) for sample  Standard Error (of the mean) (SEM) (using the sample means to estimate the population mean) is the standard deviation of all sample means (of a given size). [see Central Limit Theorem]\nσ ÷ √n Confidence Interval CI = mean ± [(z-scores for confidence level)*standard error] Z Scores z is a unit of measure that is equivalent to the number of standard deviations a value is away from the mean value.\nZ = (Data Point - Mean) / Standard Deviation Eg : if z = 1.79 , then it is 1.79 σ away from the mean* ","description":"Measures of Dispersion/Spread","id":6,"section":"handbook","tags":null,"title":"Dispersion/Spread","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\n Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae.\nNote that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item  First Sub-item Second Sub-item    Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","description":"","id":7,"section":"blog","tags":["markdown","css","html","themes"],"title":"Markdown Syntax Guide","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/markdown-syntax/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nHugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n YouTube Privacy Enhanced Shortcode   Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019 Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }  ","description":"","id":8,"section":"blog","tags":["shortcodes","privacy"],"title":"Rich Content","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/rich-content/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\ninhospita parcite confusaque translucet patri vestro qui optatis\nlumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus\nsilentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria\ntractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra\ndicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere\nfurit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli\nLelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare\nEchionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert\nausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae\nvulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem\nPropoetides parte.\n","description":"","id":9,"section":"blog","tags":["markdown","text"],"title":"Placeholder Text","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/placeholder-text/"},{"content":"Lorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nEmoji can be enabled in a Hugo project in a number of ways.\n The emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 🙈 🙉 🙉 🙊 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3  .emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }  ","description":"","id":10,"section":"blog","tags":["emoji"],"title":"Emoji Support","uri":"https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/"},{"content":"﻿# Classification\n","description":"","id":11,"section":"handbook","tags":null,"title":"Classification","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/"},{"content":"Clustering Clustering Is a set of data driven partitioning techniques designed to group a collection of objects into clusters.\n⚠️ Data should ALWAYS be continuous and standardized in nature.\n✴ Clustering is finding borders between groups\n✴ Segmentation is using borders to form groups\nApplications :  Market Segmentation Sales segmentation : what type of customer wants what Credit risk Operations : High performing persons and promotions Insurance : identifying groups with high average claim cost Data reduction : grouping observations to reduce number is obs   How to build clusters :  Select distance measure Select clustering algorithm Define the distance between 2 clusters Determine no of clusters Validate the analysis  Methods  Linkage method Variance method Centroid method  Closeness of two clusters : The decision of merging two clusters is taken on the basis of closeness of these clusters. There are multiple metrics for deciding:\n Euclidean distance: (a-b)2 = √(Σ(ai-bi)) Squared Euclidean distance: ((a-b)2)2 = Σ((ai-bi)2) Manhattan distance: (a-b)1 = Σ(ai-bi) Maximum distance: (a-b)INFINITY = maxi(ai-bi) Mahalanobis distance: √((a-b)T S-1 (-b)) {where, s : covariance matrix}    ","description":"","id":12,"section":"handbook","tags":null,"title":"Clustering","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/"},{"content":"Covariance \u0026amp; Correlation Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people.\n⚠️ Correlation doesn\u0026rsquo;t imply causation\n Covariance : Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance.\nReturns a value between -∞ \u0026amp; +∞\nSum [(X-u1) (Y-u2)] / (n-1)\n Correlation : Correlation is the standardized form of covariance.\nReturns a value between 0 (very weak) \u0026amp; ±1 (very strong), inclusive\nMulticollinearity : describes a linear relationship between two or more variables\nAutocorrelation : describes correlation of a variable with itself in a time lag\n Pearson Correlation (Karl Pearson)  Measures the strength of linear dependence between two variables A correlation of 0 doesn’t imply ‘no relationship’ between the 2 variables, it just implies ‘no linear relationship’ Calculated as the covariance of X and Y divided by product of standard deviation of X and Y Denoted as 𝝆 (rho) for population and r for sample correlation coefficient  r = i=1n[(xi- x) (yi-y)]i=1n(xi- x)i=1n(yi- y)\nSpearman Correlation  Is a non-parametric technique. Does not test a linear relationship but the strength of monotonicity between 2 variables ie the direction. Immune to outliers Can detect non-linear monotonic relationships easily as highly correlated. Divides the variable observations into ranks and runs correlation analysis based on the ranks instead of the original observation data.  However, Spearman correlation values tend to be subdued as compared to Pearson’s when detecting linear relationships: due to removal of impact of squared distances\n","description":"","id":13,"section":"handbook","tags":null,"title":"Covariance \u0026 Correlation","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/"},{"content":"Distributions Normal / Gaussian / Continuous Distribution data distributed symmetrically around the center skewness = kurtosis = 0. Mean = Median = Mode. Also known as the bell curve.\nBinomial Distribution Discrete distribution used in statistics. Only counts 2 states typically 0 and 1.\nUniform Distribution Consists of similar values throughout\nSkewed Distribution Data distributed which spikes towards either ends as opposed to the central spike in normal distribution (skewness: lack of symmetry)\nKurtosis : pointiness of the curve\nPositive kurtosis = leptokurtic | Negative kurtosis = platykurtic\n    Data dist type Measure of Central Tendency Measure of spread (variation)     Normal Mean Standard Deviation   Non normal (skewed) Median Range, Percentile \u0026amp; IQR     To convert any dataset with any mean \u0026amp; std deviation to a dataset with mean = 0 \u0026amp; std dev = 1. Can be done using z-scores: z = (x - x̄) ÷ s  Central Limit Theorem The distribution of the sample means tends towards a normal distribution as the number of samples increase\n Take a random population and plot its distribution (assuming distribution ≠ normal distribution) Samples of constant size n are to be taken from the population (n1 = n2 = n3 …) Take a random sample of size n1 from the population and calculate its mean x̄1 Take a random sample of size n2 from the population and calculate its mean x̄2 Plot the means x̄1, x̄2, x̄3 … The sampling distribution (plot of x̄1, x̄2, x̄3 …) tends to be a normal distribution as the number of samples increases The variance of the sampling distribution = The variance of the population / sample size σx̄2 = σ2 / n With a higher value of n, the sampling distribution tends to have a lower variance (high kurtosis), and vice-versa The standard deviation of the sampling means is called Standard Error of the Mean.  ","description":"","id":14,"section":"handbook","tags":null,"title":"Distributions","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/"},{"content":"﻿# Ensemble Learning\nEnsemble model combines multiple ‘individual’ (diverse) models together and delivers superior prediction power. A good model should maintain a balance between bias-variance. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis.\n**Bagging :**Bagging is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.Objective is to average noisy and unbiased models to create a model with low variance\nCreate Multiple DataSets:Sampling is done with replacement on the original data and new datasets are formed.The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging modelTaking row and column fractions less than 1 helps in making robust models, less prone to overfittingBuild Multiple Classifiers:Classifiers are built on each data set.Generally the same classifier is modeled on each data set and predictions are made.Combine Classifiers:The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand.The combined values are generally more robust than a single model.\n**Boosting :**The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.It starts by assigning equal weights to each observation.Base learning algorithm is applied. If there is a prediction error, then the misclassified observations are assigned a higher weightage.Next base learning algorithm is applied.The iteration continues until the limit of learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.It has shown better predictive accuracy than bagging, but it also tends to overfit the training data as well.\nStacking works in two phases. First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error.\n","description":"","id":15,"section":"handbook","tags":null,"title":"Ensemble Learning","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/"},{"content":"Hypothesis Testing  Null Hypothesis  H0 : μ1 = μ2x \nThe means of the two groups (μ1 \u0026amp; μ2) belong to the same population. (p \u0026gt; 0.05)\n Alternative Hypothesis  HA : μ1 ≠ μ2\nThe means of the two groups (μ1 \u0026amp; μ2) belong to different populations. In case of multiple groups, the mean of all groups shouldn’t be the same. At least one group should be different. (p \u0026lt; 0.05)\n Significance level : α The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. Typically 0.05 (5%)\nConfidence level : (1 - α)  Errors Type I : Rejecting the null hypothesis even if it\u0026rsquo;s true False Positive\nType II : Accepting the null hypothesis even when it\u0026rsquo;s false False Negative\nProbability (type I error) = α | Probability (type II error) = β\n Analogy\nPerson on trial :\nNull = He is innocent Alternate = He is guilty\nType I : reject the null when true : he is innocent but rejected : innocent person in jail ⛔ avoid\nType II : accept the null when false : he is guilty but declared innocent : guilty person free\nPerson with cancer :\nNull = Doesn't have cancer Alternate = Has cancer\nType I : reject the null when true : doesn’t have but detected : person lives\nType II : accept the null when false : has but not detected : person dies ⛔ avoid\n Misclassification Errors (Confusion Matrix)\n   Actuals ⏬ Predicted ⏩       Pred ✔️ Pred ❌   Act ❌ FP TN   Act ✔️ TP FN    ","description":"","id":16,"section":"handbook","tags":null,"title":"Hypothesis Testing","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/"},{"content":"KNN (K- Nearest Neighbors) It is a simple algorithm which classifies cases based on its votes by its nearest neighbors whose class is already known. The “K” is KNN algorithm is the number of nearest neighbors we wish to take vote from. The class to be chosen depends on a distance function.\nCategorical : Euclidean, Manhattan, Minkowski\nContinuous : Hamming\nAdvantages:  Attributes with multiple missing values can be easily treated Can predict both qualitative \u0026amp; quantitative (by taking average) attributes Easy interpretation. Low calculation time.  ","description":"","id":17,"section":"handbook","tags":null,"title":"KNN","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/"},{"content":"The Important Stuff Everyone Misses!   Multivariate Analysis is nothing but regression\n  Logistic regression IS a binomial regression (with logit link), a special case of the Generalized Linear Model. It doesn\u0026rsquo;t classify anything unless a threshold for the probability is set. Classification is just its application.\n  Stepwise regression is by no means a regression. It\u0026rsquo;s a (flawed) method of variable selection.\n  OLS is a method of estimation (among others: GLS, TLS, (RE)ML, PQL, etc.), NOT a regression.\nRidge, LASSO - it\u0026rsquo;s a method of regularization, NOT a regression.\n  There are tens of models for the regression analysis. You mention mainly linear and logistic - it\u0026rsquo;s just the GLM! Learn the others too (link in a comment). STOP with the \u0026ldquo;17 types of regression every DS should know\u0026rdquo;.BTW, there\u0026rsquo;re 270+ statistical tests. Not just t, chi2 \u0026amp; Wilcoxon\n  ","description":"","id":18,"section":"handbook","tags":null,"title":"Knowledge Repository","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/need-to-know/"},{"content":"Natural Language Processing NLP can be divided in supervised and unsupervised techniques\n Supervised:  Text Classification (Spam Recognition, labeling, etc) Spam Detection Sentiment Analysis Intent Classification Multi-Label, Multi-Class Text Classification   Unsupervised: Topic Modeling  Applications\n Sentiment Analysis Speech Recognition Chatbot Machine Translation (Google Translate) Spell Checking Keyword Search Information Extraction Advertisement Matching  NLU - Natural Language Understanding  Mapping input to useful representations Analyzing different aspects of the language  NLG - Natural Language Generation  Text planning Sentence planning Text realization  Ambiguities  Lexical Ambiguity - Two or more possible meanings in a word.  She is looking for a match (matchstick vs partner) The fisherman went to the bank (riverbank vs bank)   Syntactic Ambiguity (aka structural/grammatical ambiguity) - Two or more possible meanings in a sentence or a sequence of words.  The chicken is ready to eat   Referential Ambiguity - Referring to something using pronouns  The boy told his father about the theft. He was very upset.    (Processing) Terminologies Step1: Tokenization Process of breaking the string into tokens, which are small structures eg words and special characters.\n Break a complex sentence into words Understand the importance of each word Produce a structural description on an input sentence.  Bigrams, Trigrams, and Ngram - Token of 2,3 or n number of words written together\nfrom nltk.tokenize import word_tokenize \u0026lt;\u0026gt; = word_tokenize(\u0026lt;\u0026gt;) # check distribution for word in \u0026lt;\u0026gt; fdist=[word.lower()]+=1 fdist from nltk.util import bigrams, trigrams, ngrams singles = nltk.word_tokenize(\u0026lt;string\u0026gt;) doubles = list(nltk.bigrams(\u0026lt;string\u0026gt;)) quads = list(nltk.ngrams(\u0026lt;string\u0026gt; , 4))  Step2: Stemming Normalize words into its base or root form by cutting prefixes or suffixes. Eg affects, affected, affecting, affection are stems of affect.\nCommon Stemmers: Porter Stemmer (lenient) ; Lancaster Stemmer (aggressive) ; Snowball Stemmer (requires language input)\nfrom nltk.stem import PorterStemmer from nltk.stem import LancasterStemmer PorterStemmer.stem(\u0026lt;word\u0026gt;) LancasterStemmer.stem(\u0026lt;word\u0026gt;) #aggressive stemmer  Lemmatization Considers the morphological analysis of the word. Needs a detailed dictionary to link the word back to its lemma.\n Groups together different inflected forms of the word, called Lemma. Somehow similar to stemming as it maps several words to a common root. Output is a proper word  Eg gone going and went all maps to go\nfrom nltk.stem import wordnet from nltk.stem import WordNetLemmatizer WordNetLemmatizer.lemmatize('\u0026lt;word\u0026gt;')  Stop Words Sentence forming words not required for language processing. (eg: I, me, we, our, he, him_)_\nParts of Speech (POS) Breaking the sentence into different parts of speech (eg, Noun, verb, adverb, adjective, etc.)\nNamed Entity Recognition (NER) Connects the word to a named entity (eg: Movie, Organization, Person, Location, etc.). Knowledge Graphs are used to\nSyntax Set of rules, principle and process that govern sentence formation\nSyntax Tree: representation of syntactic structure of sentences or strings\nChunking Picking up Individual pieces of information (words, tokens) and grouping them into bigger pieces.\nTopic Modeling Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. It is an unsupervised ML technique where the model is not trained from before.\nAdvantages: Quick, easy start\nDisadvantages: Since unsupervised, lesser accuracy.\nTopic Classification: Topic modeling used with classification (supervised learning). Tags are needed to be created to predefine classes.\nMethods:\n Latent Semantic Analysis (LSA) Latent Dirichlet Allocation (LDA)  Sentiment Analysis ","description":"","id":19,"section":"handbook","tags":null,"title":"NLP","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/nlp/"},{"content":"Regression {:toc}\nRegression analysis is a statistical process for estimating the relationships among variables.\nCorrelation only measures the strength of a linear relationship, it doesn’t tell anything regarding the relationship.\nRegression is used to figure out the relationship itself.\n Eg: if correlation between Y and X is 0.7 then it says if X increases, 70% of the time Y increases. But regression tells if X increases by 1 unit, by how many units does Y increase.\n Types of Data\n Cross-sectional : data at a single point in time with multiple variables Time Series : data at multiple points in time with a single variable Longitudinal / Pooled / Panel : cross sectional time series data  Common Terms   Dependent Variable (Y) : the field which relies on other variables\n  Independent Variable(s) (X) : the assorted variables which has a direct effect on the dependent variable, whether positive or negative.\n  Linear Equation : Simply the a straight line with a formula\ny = mx + c\ny:\nm: intercept\nx: x co-ordinates\nc: constant (or error)\n  B-coefficient : if X increases by 1 unit, then Y increases by B-coeff units.\n  Intercept : Value of predicted Y if both X=0 and Y=0\nIntercept is the value or baseline, (organic growth)\n  Degrees of freedom = no of obs - (dimensions of x + dimension of y) = n- (k+1) [analogy : 5 hats]\n  Linear regression is a minimization function where the model is built to minimize the sum of squared errors whereas logistic regression is a maximization function where the model tries to maximize the parameter values of every variable in such a way that it fits very well on the data\n","description":"","id":20,"section":"handbook","tags":null,"title":"Regression","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/"},{"content":"Reinforcement Learning ","description":"","id":21,"section":"handbook","tags":null,"title":"Reinforcement Learning","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/reinforcement-learning/"},{"content":"Statistical Tests Parametric Test : used for normally distributed data (assess group means)\nNon-Parametric Test : used for skewed data (assess group medians)\nOne Tailed : uni-directional (typing speed increases with more typing) Two tailed: bi-directional (typing speed can increase or decrease with more typing)\nP-value is the probability of the sample means coming up equal to or even further away from the hypothesized population mean.\nz-Test (sample size \u0026gt; 30) Statistical calculation used to compare sample mean to population mean. Most useful when standard deviation and the sample size is known.\nThe z score tells how far, in standard deviations, a data-point is from the mean.\nt-Test (all sample size (also referred as sample size \u0026lt; 30)) To determine if there is a statistically significant difference between two sample group means. Used when population standard deviation is unknown.\nThe t-score is a ratio between the difference between the two groups and the difference within the groups\n(A large t-score tells you that the groups are different. A small t-score tells you that the groups are similar)\n- Paired samples t-Test is used when there is a before - after ideology (matched) - Independent samples t-Test (Equal Variances/Unequal Variances) is also known as a regular t-Test\nANOVA It is basically an extension to t test used when more than 2 samples are to be compared.\nIt is used to determine whether there are any statistically significant differences between the means of three or more independent groups.\nANOVA uses categorical independent variables and a continuous dependent variable\nH0 : all the means of the groups are NOT statistically different (All samples are from the same population)\nHA : at least two group means are statistically different from each other. (At least one sample comes from a different population)\nF = (Between Group Variability) ÷ (Within Group Variability)\nChi-square To test dependency or independency of categorical variables. Are 2 categorical variables independent\nAnuj says it checks the difference in means + variance\nObserved vs expected\nF test It\u0026rsquo;s a significance of variance test used for model evaluation. It is a one way ANOVA.\nIt is basically the ratio of 2 chi square tests.\n| \u0026mdash; | \u0026mdash; | \u0026mdash; | \u0026mdash; |\n| | | Unmatched | Matched |\n| Continuous Data | 2 Groups | t - Test | Paired t - Test |\n| \u0026gt; 2 Groups | ANOVA | Blocked Anova |\n| Ordinal Data | 2 Groups | Mann-Whitney U or Median Test | Wilcoxon matched pairs or signed ranks test |\n| \u0026gt; 2 Groups | Kruskal-Wallis 1-way Anova | Friedman 2-way Anov |\n| Nominal Data | 2 Groups | Fisher’s Exact Test | McNemar’s Test |\n| \u0026gt; 2 Groups | Chi-square | |\n| \u0026mdash; | \u0026mdash; |\n| Type of Test: | Use: |\n| Correlational | These tests look for an association between variables |\n| Pearson correlation | Tests for the strength of the association between two continuous variables |\n| Spearman correlation | Tests for the strength of the association between two ordinal variables (does not rely on the assumption of normal distributed data) |\n| Chi-square | Tests for the strength of the association between two categorical variables |\n| Comparison of Means: look for the difference between the means of variables |\n| Paired T-test | Tests for difference between two related variables |\n| Independent T-test | Tests for difference between two independent variables |\n| ANOVA | Tests the difference between group means after any other variance in the outcome variable is accounted for |\n| Regression: assess if change in one variable predicts change in another variable |\n| Simple regression | Tests how change in the predictor variable predicts the level of change in the outcome variable |\n| Multiple regression | Tests how change in the combination of two or more predictor variables predict the level of change in the outcome variable |\n| Non-parametric: are used when the data does not meet assumptions required for parametric tests |\n| Wilcoxon rank-sum test | Tests for difference between two independent variables - takes into account magnitude and direction of difference |\n| Wilcoxon sign-rank test | Tests for difference between two related variables - takes into account magnitude and direction of difference |\n| Sign test | Tests if two related variables are different – ignores magnitude of change, only takes into account direction |\nWhen to use what test\nCheatsheet Python tests\n","description":"","id":22,"section":"handbook","tags":null,"title":"Statistical Tests","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/"},{"content":"Singular Value Decomposition ","description":"","id":23,"section":"handbook","tags":null,"title":"SVD","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/"},{"content":"Time Series (Forecasting) [Method = Box-Jenkins (B-J)] Components Trend : A long term (relatively smooth) pattern that usually persists for more than one year\nEg : increasing no of flight passengers\nSeasonal : Pattern that occurs at regular intervals. [multiple times in a year (least is once a year)]\nEg December sale\nCyclical : Pattern that occurs over a long time, generally continues over a year\nEg : Recession\nRandom : The component obtained after the above patterns have been extracted.\nModels White Noise : Series is purely random in nature (best take average of model)\nAutoRegressive Model : (p) : Past Terms\nNo of past values to be taken into account = parameters = no of beta coefficients taken into equation.\nYt = f ( Yt-1 , Yt-2 , Yt-3 , ….. ε )\nMoving Average Model : (q) : Error Terms\nForecast by taking the error terms. The error terms are assumed to be white noise with mean zero and a constant variance.\nYt = f ( εt-1 , εt-2 , εt-3 , ….. )\nARMA Model : (p,q)\nCombines AutoRegressive and Moving Average models\nY = [ B0 + B1Yt-1 + B2Yt-2 + B3Yt-3 + εt ] + [ ϕ0 + ϕ1εt-1 , ϕ2εt-2 , ϕ3εt-3 , ….. εt-q ]\nARIMA Model : (p,d,q)\nCombines ARMA with differencing (I)\nI = 1 : yt - yt-1 // I = 2 : (yt - yt-1) - (yt-1 - yt-2) eg: 25, 16, 9, 4 : (25-16) - (16-9) : 9, 7\nARIMA (1,0,0) = AR Model ARIMA (0,0,1) = MA Model ARIMA (1,0,1) = ARMA Model ARIMA (0,1,0) = Random walk Model\nPositive auto-correlation to higher lags -\u0026gt; higher order differencing\nZero or negative for first order -\u0026gt; no differencing needed\nOptimal order of differencing -\u0026gt; best prediction\nExponential Smoothing Model :\nTakes the average of the last few observations\nStationarity![](https://lh4.googleusercontent.com/P7qQPE2R56lbKYrZQJqKjA-odnrowOTjhbLmasBIUHKvQy5-fyUB1euJXIFp4z8tiRfa5gNOlnoCqFjuB4QNUcZPuWsUju2VuLGF0MjzzUcbnXDn4BMbv6_dX9IhrOzBmVfjOGJx =101.19249841068017x40) Stationary Series : In a stationary series the mean variance and autocorrelation are all constant over time.![](https://lh4.googleusercontent.com/UqUWgR9gRptIANMGTm3Zx9Kh8LPd7wO5lwgENgLywi-AxiC8chLa8fPh4OC2yPr5kwrF1KQVELq8PW3pp2knRuSgqNWVnd_pWFE4QBw2oWwRlNgoMbdeKDBasFoqM3-CIMWMfZNY =150.60944206008583x55)![](https://lh3.googleusercontent.com/gDfimTayxUIEfOGgH6EZNn_G4n5p7lgJ7a6wkkMED_4Ozb4nRVKQhWt2yFyNwt1wZC3R-vYPt_F2x4adCk8Z28I5gcbi7UAA7xnsUCAXEQ_JBMwHlIRuBeNmS1zPbE35kSliXWbb =100.7469670710572x42)\n The mean of the series should not be a function of time. It should be a constant The variance of the series should not be a function of time The covariance of the i th term and the (i + m) th term should not be a function of time![](https://lh5.googleusercontent.com/KPxuhEbavlyk2wl16xpM-l0GLi_qf6RY73791jSJTL3Oa5H7GUtVCFEk4dxJ4UW0slKGFUIWuSkykKCXKshorir50xORUvlramcLNViXOQZEi1v-3X9eWqBOe6Le8sHbnvrmwFu7 =112.65901060070674x42)  Types of Stationarity : Strictly Stationary / Weakly Stationary / Covariance Stationary\nA series which is non stationary can be made stationary after differencing (called as Integrated) hence ARIMA is used for non stationary while ARMA for stationary variables.\nDickey Fuller Test of stationarity : if null hypothesis is rejected then we get a stationary time series.\n![](https://docs.google.com/drawings/u/0/d/sck4Fucjt68q1SJCOeBRq7Q/image?w=222\u0026amp;h=201\u0026amp;rev=278\u0026amp;ac=1\u0026amp;parent=1DFQeQ9FRGL5QV2y-d85pI0R563x5zlvv_Ln5yTr6x-w =222x201)\nThe estimation and forecasting of univariate time series is carried out by Box-Jenkins (B-J) methodology. It is only applicable to stationary variables.\n Identification  AutoCorrelation Function (ACF) : It measures a simple correlation between current observation (Yt) and the observation (p) periods from the current one (Yt-p) eg Y5 ~ Y4 Partial AutoCorrelation Function (PACF) : Used to measure the degree of association between Yt and Yt-p with the effects at intermediate time lags removed. eg Y5 ~ Y1 Inference from ACF and PACF : To see to what extent the current values of a series are related to past values.    Correlograms are plotted (plot of ACF vs lags) to find the initial values for the order of p \u0026amp; q of the AR and MA process. This is done by looking at the ACF and PACF coefficients and finding the pattern from the table\n Estimation\n Yule Walker Procedure Method of Moments Maximum Likelihood Method    Diagnostic Checking : Different methods can be obtained for various combinations of AR and MA\n Lowest value of AIC / BIC / SBIC Plot of residual ACF : should be random / no pattern in error / mean should be zero / residuals should be white noise    Check if data is regularly spaced and same intervals\n  Format dates appropriately\n  Sort the dates\n  Check for missing values\n proc expand -\u0026gt; interpolate missing values (can also aggregate data)    Original Time Series - Seasonal Component = Seasonally Adjusted\n","description":"","id":24,"section":"handbook","tags":null,"title":"Time Series / Forecasting","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/"},{"content":"Model Evaluation / Selection / Fits / Validation Train Test Split  Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs.  The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.\nRegression Metrics:  R Square: % of variance in Y that is explained by X. It is defined as the square of correlation between Predicted and Actual values.  ![](https://lh3.googleusercontent.com/q7F8E2RPDOT-odNpAUtV-NLMIEC8cIOUID53ZX_COvkIPu8gvdTy6EG-g9qXPPSp-q1jklJ8BDWBnDd1xlVoyOH_8Szch_MLG-uyub_K69ioQevL9J_QZr4P0qO_PCvtc8lxXezR =153x52)\nR2= SSEIndependent VarSSEIndependent Var + SSEErrors\n Adjusted R Square: Similar to R2. It penalizes for adding impurity (insignificant variables) to the model MSE (Mean Squared Error) : RMSE (Root Mean Square Error) : It measures standard deviation of the residuals.  Model with the least RMSE is the best model\n= sqrt (Sum of Squared Errors) / no of obs = sqrt (mean ( (Actual - Predicted)2 ))\n![](https://lh3.googleusercontent.com/BMkjNWVQCJSEuhhwB7OcrweuDf43cblmp2yL2uqnKb3PeS0U927ylIohcxuWbq9CcIN_6th0vNw38KW8hpQV1nirzTuvho95ri6DFqBfrdDe1WPEXdidt38UEuuvPBfG9Km0Lcz_ =213x59)\nMean Square : Sum of squares / df\n MAE (Mean Absolute Error) : sum( |Error| ) / n Error = Actual - Predicted |Error|=Absolute Error MAPE (Mean Absolute Percentage Error) : { absolute (average [ (Actual - Predicted) / Actual ])} should not exceed ~ 8% - 10% AIC BIC  Loss Functions: objective is to minimise these\n MAE : Mean Absolute Error (mean of the absolute errors) MSE : Mean Squared Error (mean of the squared errors) RMSE : Root Mean Squared Error (square root of the mean of squared errors)  Classification Metrics :  Precision (aka PPV) : TP ÷ (TP + FP) Recall (aka Sensitivity) : TP ÷ (TP + FN) F1 Score : 2x (Precision*Recall) ÷ (Precision+Recall)  Confusion Matrix ![](https://lh6.googleusercontent.com/V1bR60sTazQxuSSGuoaafLFL2gG53aT3Xe-b37gtsjadm_gIlgoXGGMX5zDGBpVNBOoc9jpDwpbzpVwTwiYe8R5FZZoDc9JZ3BMfgQyB_rpTb3yV0W6Y50-gE7RhqUEUQe2zAbyx =292x219) | | Actual (+) | Actual (-) |\n| Predicted (+) | TP | TP |\n| Predicted (-) | FN | TN |\nA confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data\nSensitivity : (True Positive Rate or Recall)\n% of actual +ve predicted as +ve = TP ÷ (TP + FN)\n*Specificity :\n*% of actual -ve predicted as-ve = TN ÷ (FP + TN)\nPositive Predicted Value = TP ÷ (TP + FP)\nNegative Predicted Value = TN ÷ (TN + FN)\nAccuracy = (TP + FP) ÷ (TP + FP + TN + FN)\nMisclassification Rate = (FP+FN) ÷ (TP + FP + TN + FN) = (1-Accuracy)\nBoth sensitivity and specificity should be high for a good model.\n [Best Analogy : identifying a single terrorist in a crowd and sniping him vs bombing the place\nSniping : high sensitivity ; high specificity || Bombing : high sensitivity ; low specificity]\n ![](https://lh4.googleusercontent.com/FkafGOErm0s0Xoe-A6Jz4sPwzuhkiJG5R78igy7yyF75B3dGphFuDXuCV49deEHcmBsXys33YWMcEXvaS3yt2MdGQI8K87-TMtjSUWLIJrjl0WNJgveMCP_bYtQBwk0kxKpVA1SC =234x58)\n| \u0026mdash; | \u0026mdash; | | TP | FN | | FP | TN |\n![](https://lh6.googleusercontent.com/-88NYrtKtm6jDoSaPQOuolCDT-TUMZ9JAJ2In_J5oe8qPpNl5vVH59fCvQrj2YCIqDSbXzi-Us04VI7m9gCNml_ArzF4biKv5fnRjWmutvTY2nxWcgjBdo0ILlDSQIWnmEeNaCaB =209x68)\nMedical Model : High Recall Model :: FN should be avoided\nSpam Model : High Precision Model :: FP should be avoided\nBeta Score Fβ (Range: 0 to ∞)\n(Fβ=0) Precision …… F0.5 ……. F1 …….. F2 ……. Recall (Fβ=∞)\nFor other values of β, if they are close to 0, we get something close to precision, if they are large numbers, then we get something close to recall, and if β=1, then we get the harmonic mean of precision and recall.\nFor the spaceship model, we can\u0026rsquo;t really afford any malfunctioning parts, and it\u0026rsquo;s ok if we overcheck some of the parts that are working well. Therefore, this is a high recall model, so we associate it with beta = 2.\nFor the notifications model, since it\u0026rsquo;s free to send them, we won\u0026rsquo;t get harmed too much if we send them to more people than we need to. But we also shouldn\u0026rsquo;t overdo it, since it will annoy the users. We also would like to find as many interested users as we can. Thus, this is a model which should have a decent precision and a decent recall. Beta = 1 should work here.\nFor the Promotional Material model, since it costs us to send the material, we really don\u0026rsquo;t want to send it to many people that won\u0026rsquo;t be interested. Thus, this is a high precision model. Thus, beta = 0.5 will work here.\nROC (Receiver Operating Characteristics) Curve![](https://lh6.googleusercontent.com/R_1QXnZi0zNM6jWBEZNXpxStbyq-3GVzOCmODRvJSHYNXfPDmEGUgEUsUWFenRl3lELiF4-VZ55Q924V9p0G9dvJzmmw9TbQ8iVJBAxp3IwuhYSbEh8Dr5ZUdrg7q-Wf1QkBvPj3 =302x189) The ROC chart is plotted with :\nY axis as sensitivity [True Positive Rate] and\nX axis as (1-specificity) [False Positive Rate]\nThe diagonal line represents a random chance model whereas the line curved towards top left represents how better the model is. The more distant the curve, from the random chance line, the better is the model.\nAUC : The area under the curve is also used as a quality measure. A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1\nc = (%concordant - %discordant + %tied) ÷ no of pairs\n(random chance) 0.5 \u0026lt; c-stat \u0026lt; 1 (perfect model)\nK-S Chart![](https://lh5.googleusercontent.com/tvo-uQPoquKBLLN-vNNiApTart8yLaDVtNskblYlqjZcE1dc2qIAEEWSbPuQ9RFnAVKAJRrdW96ENQe9vdybKCnXneAm0TNYar9bzs4QGApe-wjqK6sD79J63vdlE8eBSdvJXCgZ =297x174) K-S is a measure of the degree of separation between the positive and negative distributions (events vs non events). It is defined as the maximum difference between cumulative % event and cumulative % non event.\nThe K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. If the model selects cases randomly from the population, the K-S would be 0.\nKS Statistic is the measure of maximum separability.\n(random chance) 0 \u0026lt; K-S \u0026lt; 100 (perfect model)\nEXAMPLE\nGain and Lift Charts Gain is defined as the cumulative % of target (the ratio of cumulative number of targets (events), to the total number of targets (events))\nLift measures how much better one can expect to do with the predictive model comparing without a model. It is the ratio of gain % to the random expectation % at a given decile level. The random expectation at the xth decile is x%.\n Score (predicted probability) the validation sample using the response model under consideration. Rank the scored file, in descending order by estimated probability Split the ranked file into 10 sections (deciles)  Rank Ordering Same deciles table is created and % of Events is checked to see if there is any discrepancy in downward trend. If the trend is broken then the rank ordering is not maintained.\nConcordance![](https://lh6.googleusercontent.com/q5z3xrwoK98656UAitBaFqZ1qLWa4G_8GLXt5HfLfHmwykQ33uKfpnVA0QzvcRxiv2rwbFaW7ojeVw1jm5-a-KCo_2ob2HNgX4XySmfGFRBDcI5xlBa8mD8e_bZ-bq-FUKy-zQP8 =151x105)  Concordant Pair Discordant Pair Tied Pair  Percent Concordant = (Number of concordant pairs) / Total number of pairs\nPercent Discordance = (Number of discordant pairs) / Total number of pairs\nPercent Tied = (Number of tied pairs) / Total number of pairs\nArea under curve (c statistics) = Percent Concordant + (0.5 * Percent Tied)\nHosmer and Lemeshow Goodness of Fit It is carried out to check if the regression explains the variance in data. It sorts the predicted values and groups them into 10 deciles and compares it against the sorted and grouped dependent variable in the original dataset and finds out if any significant difference between observed and expected.\nChi sq test for difference between observed and expected. High p-value indicates good fit.\nH0 : The data are consistent with a specified distribution.\nHA : The data are not consistent with a specified distribution.\nAIC [Akaike Information Criterion] By adding more parameters, better fit is obtained. However, after a certain point, model tends to overfit.\nAIC captures tradeoff between number of parameters added and the incremental amount of error.\nBasically, it penalizes models for overfitting of data. Also known as loss criterion.\nModel with lower AIC is better !\nAIC = - ln L + p L = likelihood p = parameters\nAIC = N ln (SSerror ÷ N) + 2K N = no of obs K = no of parameters fit+1\nBIC [Bayesian Information Criterion] BIC penalizes the number of parameters more than AIC. Emphasizes simplicity of model.\nGini (Somers’ D) Determines the strength and direction of relationships between pairs of variables.\nIt ranges from -1 (all pairs disagree) to +1 (all pairs agree). Should be gt 0.4\nSomer\u0026rsquo;s D = 2 AUC - 1 = (%concordant - %discordant) / 100\n= (Concordant pairs - Discordant pairs ) / Total Pairs\nGamma Similar to Somers’ D but does not penalize for tied pairs.\nBias - Variance Tradeoff\nBias : how much on average is my predicted values different from actual values. High bias is underfitting.\nVariance : how different will predictions be, at the same point, if different samples are taken from the same population. High variance causes overfitting.\n![](https://lh5.googleusercontent.com/QNFSL6we1fCDjTMHUUqz-WnKg_JuldfEulTu2qvDX7ZoDgoSxFAVhejDThCMLTRzfDqRs7Xll9ECL6NU7x6joEWnY0KXJsEePFabL5cH9pq0k6ya3Lxfyt9iguICnf7PfdXxlX2F =286x170)\nhigh bias = high (pred - act) = high error = underfit model\nhigh variance = high sensitivity in changes in data = overfit model\nWhen a model has high bias, this means that it doesn\u0026rsquo;t do a good job of bending to the data\nWhen a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset\nHigh Bias, Low Variance models tend to underfit data, as they are not flexible. Linear models fall into this category of models.\nHigh Variance, Low Bias models tend to overfit data, as they are too flexible. Decision trees fall into this category of models.\n![](https://lh4.googleusercontent.com/ga9WJ0VSA54deurET54OU6jSsyfQcOFCfABQlXpqVonjll7dvCCz_17NvBrfxrAYS1uXR4XYxDkWoeDkxsYgvirMNjbATe8b1Yjvm2YfhCZ-ZzmSQFAKLSEvv0j6KSjD7XfjqQN- =492x246)\nModel Complexity Graph : Plots the training and testing error and helps find the optimal point between underfitting and overfitting.\n   Validation : K-fold cross Validation  Sample is partitioned into k equal sized subsamples. A single subsample is used as the validation dataset for testing the model. Remaining k-1 subsamples are used as training data. The cross validation process is repeated k times.  Advantages\nAll observations are used for both training and validation.\nEach of the k subsamples used exactly once as the validation data.\nLearning Curves ![](https://lh5.googleusercontent.com/zHfafLEPe9jlrCSeRDQFivuKWDhpNAMsltF8Ulh0-bRd0ANAXuWsrsSn5lyUJI30q2l7Q_sx3-1Qae7Xgr3MmSf53yi7tAPxnCW4oRmrXvxWRgG_F3yooM3f19eYF4DzT1u0SUqZ =427x181)\nGrid Search Technique used to list down all the possibilities of the hyperparameters and pick the best one.\nMODEL MONITORING\nPopulation Stability Index (PSI)\n Sort scoring variable on descending order in scoring sample Split the data into 10 or 20 groups (deciling) Calculate % of records in each group based on scoring sample Calculate % of records in each group based on training sample Calculate difference between Step 3 and Step 4 Take Natural Log of (Step3 / Step4) Multiply Step5 and Step6  continue model \u0026lt; 0.1 \u0026lt; slight change \u0026lt; 0.2 \u0026lt; significant change\n","description":"","id":25,"section":"handbook","tags":null,"title":"☑️ Model Evaluation","uri":"https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/model-evaluation/"}]