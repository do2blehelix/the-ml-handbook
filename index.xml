<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/</link><description>Recent content in Home on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Fri, 28 Feb 2020 10:08:56 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/index.xml" rel="self" type="application/rss+xml"/><item><title>K-Means</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</guid><description>K means (Non Hierarchical) from sklearn.cluster import KMeans It is based on division of objects into non overlapping subsets. Main objective is to form clusters that are homogeneous in nature and heterogeneous to each other.
❕ Only for continuous variables.
Advantages Faster, more reliable, works with large data. Computationally lighter than other methods Disadvantages Can only identify clusters circular / spherical in nature. (check crescent dataset) Distance based Process Identify value of ‘k’ Assign random k observations as seeds Assign each record to one of the k seeds based on proximity Form clusters Calculate centroids of clusters Assign centroids as new seed Form new clusters Recalculate clusters Continue process until stable clusters are formed (boundary ceases to change) Elbow Criterion (Scree Plot): K means clustering doesn&amp;rsquo;t provide an estimate of the number of clusters required.</description></item><item><title>Bagging (Random Forest)</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</guid><description>Boosted Aggregating Bagging from sklearn.ensemble import BaggingClassifier Bagging is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.
Objective is to average noisy and unbiased models to create a model with low variance
Algorithm Create Multiple Datasets: Sampling is done with replacement on the original data and new datasets are formed. The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting Build Multiple Classifiers: Classifiers are built on each data set.</description></item><item><title>☑️ Regression Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</guid><description>Regression Metrics: R Square: % of variance in Y that is explained by X. It is defined as the square of correlation between Predicted and Actual values.
R2= SSEIndependent VarSSEIndependent Var + SSEErrors
Adjusted R Square: Similar to R2. It penalizes for adding impurity (insignificant variables) to the model
MSE (Mean Squared Error) : Sum of squares / df
RMSE (Root Mean Square Error) : It measures standard deviation of the residuals.</description></item><item><title>Logistic Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/1-logistic-regression/</link><pubDate>Wed, 21 Oct 2020 06:14:22 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/1-logistic-regression/</guid><description>Logistic Regression from sklearn.linear_model import LogisticRegression Although Logistic Regression is termed a Regression, it is in fact a method of Classification. The underlying methodology utilized concepts to linear regression and hence the name sayed.
Method = Maximum Likelihood Estimation / Chi-square
Logistic Regression is used to model the probability of an outcome. It is based on concept of Generalized Linear Model (GLM).
Dependent Variable Y = Binary (eg 0 or 1)</description></item><item><title>Using user defined libraries in python</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/</link><pubDate>Tue, 20 Oct 2020 15:12:15 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/</guid><description>Python Check Python version
python --version By default the python stores its packages in libraries</description></item><item><title>Central Tendency</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</guid><description>Measures of Central Tendency Mean The sum total of units divided by the number of units. Average
μ (population) | x̄ (sample) | 1/ni=1nxi
Median The middle / midpoint value in a sorted sequence. Middle
It divides the data 50(more):50(less).
Mode The most commonly occurring value Frequent
Two modes = Bimodal ; Multiple modes = Multimodal
  Q) Why do we use mean most times ?
A) It takes all values into consideration.</description></item><item><title>Linear Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</guid><description>Linear Regression Linear Regression is the most basic type of regression that there is. It takes a variable and states that on the basis of other variables, it will predict the concerned variable. It does this by simply drawing a line through the points and generating an equation.
Method = Ordinary Least Square
⚠️ Linear Regression requires the complete data should be numerical in nature.
Dependent Variable Y: the one you need to predict Independent Variable X: the others with which you will predict Residual: difference between observation and the fitted line Error: Residual The objective is to minimize the sum of squares of the residuals (difference between observation and the fitted line)</description></item><item><title>The Important Stuff Everyone Misses</title><link>https://do2blehelix.github.io/the-ml-handbook/information/important-stuff-everyone-misses/</link><pubDate>Tue, 28 Jan 2020 00:10:48 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/important-stuff-everyone-misses/</guid><description>Easy to Overlook these Multivariate Analysis is nothing but regression
Logistic regression IS a binomial regression (with logit link), a special case of the Generalized Linear Model. It doesn&amp;rsquo;t classify anything unless a threshold for the probability is set. Classification is just its application.
Stepwise regression is by no means a regression. It&amp;rsquo;s a (flawed) method of variable selection.
OLS is a method of estimation (among others: GLS, TLS, (RE)ML, PQL, etc.</description></item><item><title>Hierarchical Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</guid><description>Hierarchical Clustering It is a set of nested clusters organized as a hierarchical tree. No decision about number of clusters It is not used when data is big due to higher processing time.
Types Agglomerative : Start from n clusters and get to one cluster Bottom up approach Divisive : Start from one cluster and get to n clusters Top down approach Advantages Produces an additional ability to visualize Potent, especially if the data contains real hierarchical relationships (eg evolution) Disadvantages Computationally intensive Sensitive to noise and outliers Distance Between Clusters (Agglomerative Clustering) Single link : Shortest distance between an element in one cluster and an element in another cluster.</description></item><item><title>Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</guid><description>Boosting The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.
It starts by assigning equal weights to each observation. Base learning algorithm is applied. If there is a prediction error, then the misclassified observations are assigned a higher weightage. Next base learning algorithm is applied. The iteration continues until the limit of learning algorithm is reached or higher accuracy is achieved. Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model.</description></item><item><title>Decision Trees</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/2-decision-trees/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/2-decision-trees/</guid><description>Decision Trees from sklearn.tree import DecisionTreeClassifier It&amp;rsquo;s a type of supervised learning algorithm mostly used for classification problems. Works for both categorical and continuous variables.
The population is split into two or more homogeneous sets based on the splitting criterion.
Analogy : They ask a bunch of questions to arrive at a particular answer. (20 Questions)
Types :
Categorical Variable Decision Tree : Categorical Target Variable Continuous Variable Decision Tree : Continuous Target Variable Advantages Glass-box model.</description></item><item><title>Dispersion/Spread</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</guid><description>Measures of Dispersion/Spread Name Spread Range Max - Min value Percentile Divides the data into 100 equal parts using 99 points Decile Divides the data into 10 equal parts using 9 points Quartile Divides the data into 4 equal parts using 3 points (25%, 50%, 75%) Median is the 2nd quartile (50%) . Interquartile range : 25% - 75% Boxplot uses a limit of 1.</description></item><item><title>Types of Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</guid><description>Types of Regression Simple Linear Regression It is the simplest form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)
Multiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It has one Dependent Variable and Multiple Independent Variables.</description></item><item><title>Data Science Project Charter</title><link>https://do2blehelix.github.io/the-ml-handbook/information/data-science-project-charter/</link><pubDate>Tue, 28 Jan 2020 00:10:48 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/data-science-project-charter/</guid><description>Data Science Project Charter 1. Data Gathering Data Gathering and dictionary creation. Data understanding plays a key role to understand what the data means.
2. Exploratory Data Analysis EDA is divided into 2 phases
Univariate Analysis Analyze and understand the ranges and categories of variables and check their distributions.
Bivariate Analysis Plot the variables of interest against the dependent variable.
Correlation Generate a correlation heatmap to figure out the highly correlated independent variables.</description></item><item><title>Stacking</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</guid><description>Stacking Stacking works in two phases.
First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error.</description></item><item><title>Support Vector Machine</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/3-support-vector-machine/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/3-support-vector-machine/</guid><description>SVM (Support Vector Machine) from sklearn.svm import SVC In this algorithm, each data item is plotted as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.
The aim is to determine the location of decision boundaries also known as hyperplane that produce the optimal separation of classes. Multiple frontiers are produced to suit the data.</description></item><item><title>Distributions</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</guid><description>Distributions Normal / Gaussian / Continuous Distribution data distributed symmetrically around the center skewness = kurtosis = 0. Mean = Median = Mode. Also known as the bell curve.
Binomial Distribution Discrete distribution used in statistics. Only counts 2 states typically 0 and 1.
Uniform Distribution Consists of similar values throughout
Skewed Distribution Data distributed which spikes towards either ends as opposed to the central spike in normal distribution (skewness: lack of symmetry)</description></item><item><title>Regularization</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</guid><description>Regularization Ridge Regression Lasso Regression</description></item><item><title>Python Environments</title><link>https://do2blehelix.github.io/the-ml-handbook/information/create-and-use-anaconda-environments/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/create-and-use-anaconda-environments/</guid><description>Python Environments Anaconda Environments provide a way of working with python packages. You can have any version of any package installed in a specific environment without changing you original set of packages. It is especially helpful for installing version compatible packages together.
Create the environment Anaconda conda create --name &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Python python -m venv &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Check if environment was created Anaconda conda env list Python Activate the environment Anaconda conda activate &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Python .</description></item><item><title>Naive Bayes</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/4-na%C3%AFve-bayes/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/4-na%C3%AFve-bayes/</guid><description>Naïve Bayes from sklearn.naive_bayes import GaussianNB It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
P(c|x) is the posterior probability of class (target) given predictor (attribute). P(c) is the prior probability of class. P(x|c) is the likelihood which is the probability of predictor given class.</description></item><item><title>Covariance &amp; Correlation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</guid><description>Covariance &amp;amp; Correlation Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people.
⚠️ Correlation doesn&amp;rsquo;t imply causation
Covariance : Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance.
Returns a value between -∞ &amp;amp; +∞
Sum [(X-u1) (Y-u2)] / (n-1)</description></item><item><title>Artificial Neural Network</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/5-artificial-neural-network/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/5-artificial-neural-network/</guid><description>Artificial Neural Network (ANN) Artificial neural networks (ANNs) are types of computer architecture inspired by biological neural networks (Nervous systems of the brain) and are used to approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are presented as systems of interconnected “neurons” which can compute values from inputs and are capable of machine learning as well as pattern recognition due their adaptive natureAn artificial neural network operates by creating connections between many different processing elements each corresponding to a single neuron in a biological brain.</description></item><item><title>☑️ Model Evaluation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</guid><description>Model Evaluation / Selection / Fits / Validation Train Test Split Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs. The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.</description></item><item><title>Hypothesis Testing</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</guid><description>Hypothesis Testing It the the action of declaring a hypothesis (a fundamental logical component) and proving its occurrence of proving its non-existence.
Types of Hypothesis Null Hypothesis H0 : μ1 = μ2x
The means of the two groups (μ1 &amp;amp; μ2) belong to the same population. (p &amp;gt; 0.05)
Alternative Hypothesis HA : μ1 ≠ μ2
The means of the two groups (μ1 &amp;amp; μ2) belong to different populations.</description></item><item><title>Shareable Python Environments</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/using-python-embeddable-to-deploy-locally/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/using-python-embeddable-to-deploy-locally/</guid><description>Install PIP and Packages in Python Embeddable Download Python Embeddable Python Embeddable is a lite flavor of python with barebone necessities. It can be used to deploy web-apps to systems which don&amp;rsquo;t have permission to install python or its packages.
[Download it from Here: Official Python 3.8.9](https://www.python.org/ftp/python/3.8.9/python-3.8.9-embed-amd64.zip)
Configure PIP Even if explicitly stated that the embeddable version of Python does not support Pip, it is possible with care. You need to:</description></item><item><title>Article Resources</title><link>https://do2blehelix.github.io/the-ml-handbook/information/article-resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/article-resources/</guid><description>Resources Here&amp;rsquo;s a list of useful resources
[Creating virtual Environments](https://towardsdatascience.com/virtual-environments-104c62d48c54) [Pip install in Python Embed](https://stackoverflow.com/a/48906746)</description></item><item><title>DBSCAN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</guid><description>Density Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN clusters densely packed points and labels the other points as noise.
Advantages: No specification of no of clusters Flexibility in shapes and sizes of clusters Able to deal with noise and outliers Disadvantages: Border points are assigned to whichever clusters find them first Faces difficulties in finding clusters of varying densities.
(variation of DBSCAN, HDBSCAN can be used to rectify) Theory: Epsilon (ε) = Search Distance around a point</description></item><item><title>Gaussian Mixture Model</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</guid><description>Gaussian Mixture Model Clustering Each point belongs to every cluster, but has a different level of membership. It assumes that each cluster has a certain statistical distribution.
Expectation - Maximization Algorithm
Advantages : Soft Clustering, sample members of multiple clusters. Cluster shape flexibility. (cluster can contain another cluster inside of it) Disadvantages : Sensitive to initialization values Slow convergence rate Possible to converge to a local optimum Process: Initialize k Gaussian distributions Soft cluster the data into the Gaussian distributions (Expectation step) Re-estimate the Gaussian (Maximization step) Evaluate log-likelihood to check for convergence Repeat from Step.</description></item><item><title>KNN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</guid><description>KNN (K- Nearest Neighbors) It is a simple algorithm which classifies cases based on its votes by its nearest neighbors whose class is already known. The “K” is KNN algorithm is the number of nearest neighbors we wish to take vote from. The class to be chosen depends on a distance function.
Categorical : Euclidean, Manhattan, Minkowski
Continuous : Hamming
Advantages: Attributes with multiple missing values can be easily treated Can predict both qualitative &amp;amp; quantitative (by taking average) attributes Easy interpretation.</description></item><item><title>NLP</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/natural-language-processing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/natural-language-processing/</guid><description>Natural Language Processing NLP can be divided in supervised and unsupervised techniques
Supervised: Text Classification (Spam Recognition, labeling, etc) Spam Detection Sentiment Analysis Intent Classification Multi-Label, Multi-Class Text Classification Unsupervised: Topic Modeling Applications
Sentiment Analysis Speech Recognition Chatbot Machine Translation (Google Translate) Spell Checking Keyword Search Information Extraction Advertisement Matching NLU - Natural Language Understanding Mapping input to useful representations Analyzing different aspects of the language NLG - Natural Language Generation Text planning Sentence planning Text realization Ambiguities Lexical Ambiguity - Two or more possible meanings in a word.</description></item><item><title>Others</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</guid><description>BIRCH</description></item><item><title>Spectral Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</guid><description>Spectral Clustering It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.</description></item><item><title>Statistical Tests</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</guid><description>Statistical Tests Statistical tests are conducted to accept or reject hypothesis. They can be divided into tests catering to specific distributions of data.
Parametric Test : used for normally distributed data (assess group means) Non-Parametric Test : used for skewed data (assess group medians) The tests are also classified according to their tails.
One Tailed : uni-directional (typing speed increases with more typing) Two tailed: bi-directional (typing speed can increase or decrease with more typing) ⚠ P-value is the probability of the sample means coming up equal to or even further away from the hypothesized population mean.</description></item><item><title>SVD</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</guid><description>Singular Value Decomposition</description></item><item><title>Time Series / Forecasting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</guid><description>Time Series (Forecasting) [Method = Box-Jenkins (B-J)] Components Trend : A long term (relatively smooth) pattern that usually persists for more than one year
Eg : increasing no of flight passengers
Seasonal : Pattern that occurs at regular intervals. [multiple times in a year (least is once a year)]
Eg December sale
Cyclical : Pattern that occurs over a long time, generally continues over a year
Eg : Recession
Random : The component obtained after the above patterns have been extracted.</description></item><item><title>☑️ Cluster Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</guid><description>Cluster Validation Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item></channel></rss>