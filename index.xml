<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The Machine Learning Handbook on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/</link><description>Recent content in The Machine Learning Handbook on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Sun, 26 Jan 2020 04:15:05 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/index.xml" rel="self" type="application/rss+xml"/><item><title>K-Means</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</guid><description>K means (Non Hierarchical) from sklearn.cluster import KMeans It is based on division of objects into non overlapping subsets. Main objective is to form clusters that are homogeneous in nature and heterogeneous to each other.
❕ Only for continuous variables.
Advantages Faster, more reliable, works with large data. Computationally lighter than other methods Disadvantages Can only identify clusters circular / spherical in nature. (check crescent dataset) Distance based Process Identify value of ‘k’ Assign random k observations as seeds Assign each record to one of the k seeds based on proximity Form clusters Calculate centroids of clusters Assign centroids as new seed Form new clusters Recalculate clusters Continue process until stable clusters are formed (boundary ceases to change) Elbow Criterion (Scree Plot): K means clustering doesn&amp;rsquo;t provide an estimate of the number of clusters required.</description></item><item><title>Bagging (Random Forest)</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/random-forest/</guid><description>Boosted Aggregating Bagging from sklearn.ensemble import BaggingClassifier Bagging is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.
Objective is to average noisy and unbiased models to create a model with low variance
Algorithm Create Multiple Datasets: Sampling is done with replacement on the original data and new datasets are formed. The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting Build Multiple Classifiers: Classifiers are built on each data set.</description></item><item><title>☑️ Regression Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</guid><description>Regression Metrics: R2 (coefficient of determination) R Square % of variance in Y that is explained by X. It is defined as the square of correlation between Predicted and Actual values.
R2= SSEIndependent VarSSEIndependent Var + SSEErrors
Adjusted R Square Similar to R2. It penalizes for adding impurity (insignificant variables) to the model
Squared Error MSE (Mean Squared Error) Sum of squares / degree of freedom
RMSE (Root Mean Square Error) It measures standard deviation of the residuals.</description></item><item><title>Logistic Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/logistic-regression/</link><pubDate>Wed, 21 Oct 2020 06:14:22 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/logistic-regression/</guid><description>Logistic Regression from sklearn.linear_model import LogisticRegression Although Logistic Regression is termed a Regression, it is in fact a method of Classification. The underlying methodology utilized concepts to linear regression and hence the name sayed.
Method = Maximum Likelihood Estimation / Chi-square
Logistic Regression is used to model the probability of an outcome. It is based on concept of Generalized Linear Model (GLM).
Dependent Variable Y = Binary (eg 0 or 1)</description></item><item><title>Python Handy Utility Codes</title><link>https://do2blehelix.github.io/the-ml-handbook/information/python-handy-commands/</link><pubDate>Tue, 20 Oct 2020 15:12:15 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/python-handy-commands/</guid><description>Python Check Python version
python --version By default the python stores its packages in libraries</description></item><item><title>Central Tendency</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</guid><description>Measures of Central Tendency Central tendency refers to the definition of a distribution of numbers by a particular central number. Generally speaking, it&amp;rsquo;s the most common number or most probable common number among a range of numbers.
If you&amp;rsquo;ve ever used the word &amp;lsquo;average&amp;rsquo; in a sentence, eg On an average the temperature is 25 degrees here, you are more than familiar with the concept already.
Mean, Median and Mode are are the top 3 measures of central tendency.</description></item><item><title>Linear Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/linear-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/linear-regression/</guid><description>Linear Regression Linear Regression is the most basic type of regression that there is. It takes a variable and states that on the basis of other variables, it will predict the concerned variable. It does this by simply drawing a line through the points and generating an equation.
Method = Ordinary Least Square
⚠️ Linear Regression requires the complete data should be numerical in nature.
Dependent Variable Y: the one you need to predict Independent Variable X: the others with which you will predict Residual: difference between observation and the fitted line Error: Residual The objective is to minimize the sum of squares of the residuals (difference between observation and the fitted line)</description></item><item><title>The Important Stuff Everyone Misses</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/important-stuff-everyone-misses/</link><pubDate>Tue, 28 Jan 2020 00:10:48 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/important-stuff-everyone-misses/</guid><description>Easy to Overlook these Multivariate Analysis is nothing but regression
Logistic regression IS a binomial regression (with logit link), a special case of the Generalized Linear Model. It doesn&amp;rsquo;t classify anything unless a threshold for the probability is set. Classification is just its application.
Stepwise regression is by no means a regression. It&amp;rsquo;s a (flawed) method of variable selection.
OLS is a method of estimation (among others: GLS, TLS, (RE)ML, PQL, etc.</description></item><item><title>Hierarchical Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</guid><description>Hierarchical Clustering It is a set of nested clusters organized as a hierarchical tree. No decision about number of clusters It is not used when data is big due to higher processing time.
Types Agglomerative : Start from n clusters and get to one cluster Bottom up approach Divisive : Start from one cluster and get to n clusters Top down approach Advantages Produces an additional ability to visualize Potent, especially if the data contains real hierarchical relationships (eg evolution) Disadvantages Computationally intensive Sensitive to noise and outliers Distance Between Clusters (Agglomerative Clustering) Single link : Shortest distance between an element in one cluster and an element in another cluster.</description></item><item><title>Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/gradient-boosting/</guid><description>Boosting The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.
It starts by assigning equal weights to each observation. Base learning algorithm is applied. If there is a prediction error, then the misclassified observations are assigned a higher weightage. Next base learning algorithm is applied. The iteration continues until the limit of learning algorithm is reached or higher accuracy is achieved. Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model.</description></item><item><title>Decision Trees</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/decision-trees/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/decision-trees/</guid><description>Decision Trees from sklearn.tree import DecisionTreeClassifier It&amp;rsquo;s a type of supervised learning algorithm mostly used for classification problems. Works for both categorical and continuous variables.
The population is split into two or more homogeneous sets based on the splitting criterion.
Analogy : They ask a bunch of questions to arrive at a particular answer. (20 Questions)
Types :
Categorical Variable Decision Tree : Categorical Target Variable Continuous Variable Decision Tree : Continuous Target Variable Advantages Glass-box model.</description></item><item><title>☑️ Classification Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</guid><description>Classification Metrics : Confusion Matrix Actual (+) Actual (-) Predicted (+) TP FP Predicted (-) FN TN A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data
Misclassifications (notified on predicted value)
False Positive = detected positive, wrongly False Negative = detected negative, wrongly The most common evaluation metrics are as below: Precision (aka PPV) : TP ÷ (TP + FP) Out of the total predicted positive how many are positive</description></item><item><title>Dispersion/Spread</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</guid><description>Measures of Dispersion/Spread While central tendency represents a range of numbers with a single value (except in some cases), the measures of dispersion shows us how evenly the data are spread.
Standard Deviation (σ) Standard Deviation, put simply shows how much the data deviates from the mean. It represents how far the data are dispersed from the mean. How much the members of a group differ from the mean value for the group.</description></item><item><title>Types of Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/multiple-polynomial-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/multiple-polynomial-regression/</guid><description>Types of Regression Simple Linear Regression It is the simplest form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)
Multiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It has one Dependent Variable and Multiple Independent Variables.</description></item><item><title>Data Science Project Charter</title><link>https://do2blehelix.github.io/the-ml-handbook/information/data-science-project-charter/</link><pubDate>Tue, 28 Jan 2020 00:10:48 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/data-science-project-charter/</guid><description>Data Science Project Charter 1. Data Gathering Data Gathering and dictionary creation. Data understanding plays a key role to understand what the data means.
2. Exploratory Data Analysis EDA is divided into 2 phases
Univariate Analysis Analyze and understand the ranges and categories of variables and check their distributions.
Bivariate Analysis Plot the variables of interest against the dependent variable.
Correlation Generate a correlation heatmap to figure out the highly correlated independent variables.</description></item><item><title>Stacking</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</guid><description>Stacking Stacking works in two phases.
First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error.</description></item><item><title>Support Vector Machine</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/support-vector-machine/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/support-vector-machine/</guid><description>SVM (Support Vector Machine) from sklearn.svm import SVC In this algorithm, each data item is plotted as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.
The aim is to determine the location of decision boundaries also known as hyperplane that produce the optimal separation of classes. Multiple frontiers are produced to suit the data.</description></item><item><title>Distributions</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</guid><description>Distributions Now that we&amp;rsquo;ve seen the central point of a distribution, and then the deviation of data from the central point, let&amp;rsquo;s explore how these distributions look like.
Before we explore the distributions, its necessary to understand a few key terminologies
Skewness Skewness is the lack of symmetry of a distribution
Positive skewed = data is heavy towards the left ; tail towards the right Negative skewed = data is heavier towards the right ; tail towards the left Kurtosis Kurtosis simply refers to the pointiness of the curve.</description></item><item><title>Regularization</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/ridge-lasso-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/ridge-lasso-regression/</guid><description>Regularization Ridge Regression Lasso Regression</description></item><item><title>☑️ Cluster Validation Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</guid><description>Cluster Validation Metrics Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item><item><title>Python &amp; Conda Environments</title><link>https://do2blehelix.github.io/the-ml-handbook/information/create-and-use-anaconda-environments/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/create-and-use-anaconda-environments/</guid><description>Python Environments Anaconda Environments provide a way of working with python packages. You can have any version of any package installed in a specific environment without changing you original set of packages. It is especially helpful for installing version compatible packages together.
Create the environment Anaconda conda create --name &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Python python -m venv &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Check if environment was created Anaconda conda env list Python Activate the environment Anaconda conda activate &amp;lt;&amp;lt;myenv&amp;gt;&amp;gt; Python .</description></item><item><title>Model Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</guid><description>Model Evaluation / Selection / Fits / Validation Train Test Split Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs. The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.</description></item><item><title>Naive Bayes</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/na%C3%AFve-bayes/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/na%C3%AFve-bayes/</guid><description>Naïve Bayes from sklearn.naive_bayes import GaussianNB It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
P(c|x) is the posterior probability of class (target) given predictor (attribute). P(c) is the prior probability of class. P(x|c) is the likelihood which is the probability of predictor given class.</description></item><item><title>Covariance &amp; Correlation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</guid><description>Covariance &amp;amp; Correlation Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people.
⚠️ Correlation doesn&amp;rsquo;t imply causation
Covariance : Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance.
Returns a value between -∞ &amp;amp; +∞
Sum [(X-u1) (Y-u2)] / (n-1)</description></item><item><title>Other Algorithms</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/other-types-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/other-types-regression/</guid><description>Various Algorithms for Regression As you will see later, there are many algorithms for classification eg decision trees, svm, etc. But what we ignore is that most of these algorithms pack in a regressor along with their classifier as well. Here&amp;rsquo;s some of the classification algorithms which are good fore regression as well.
Decision Tree Regressor from sklearn.tree import DecisionTreeRegressor Read about the classifier algorithm here
SVM Regressor from sklearn.</description></item><item><title>Artificial Neural Network</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/artificial-neural-network/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/artificial-neural-network/</guid><description>Artificial Neural Network (ANN) Artificial neural networks (ANNs) are types of computer architecture inspired by biological neural networks (Nervous systems of the brain) and are used to approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are presented as systems of interconnected “neurons” which can compute values from inputs and are capable of machine learning as well as pattern recognition due their adaptive natureAn artificial neural network operates by creating connections between many different processing elements each corresponding to a single neuron in a biological brain.</description></item><item><title>Model Choice</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</guid><description>Choice of Model Metrics Precision vs Recall Model Misclassifications (notified on predicted value)
False Positive = detected positive, wrongly False Negative = detected negative, wrongly Examples of wrong outcomes:
If an important mail is detected spam positive (FP), it is a bad outcome If a cancer patient is detected cancer negative (FN), it is a bad outcome We want to reduce the no of bad outcomes. To reduce the no of bad outcomes, we need the bad outcomes in the denominator.</description></item><item><title>Hypothesis Testing</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</guid><description>Hypothesis Testing It the the action of declaring a hypothesis (a fundamental logical component) and proving its occurrence of proving its non-existence.
Types of Hypothesis Null Hypothesis H0 : μ1 = μ2x
The means of the two groups (μ1 &amp;amp; μ2) belong to the same population. (p &amp;gt; 0.05)
Alternative Hypothesis HA : μ1 ≠ μ2
The means of the two groups (μ1 &amp;amp; μ2) belong to different populations.</description></item><item><title>Other Types</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</guid><description>Spectral Clustering It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.
BIRCH</description></item><item><title>AWS Infrastructure Essentials</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/aws-infrastructure-essentials/</link><pubDate>Tue, 04 May 2021 13:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/aws-infrastructure-essentials/</guid><description>AWS Infrastructure Essentials AWS stands for Amazon Web Services. It is one of the leading platforms for data hosting and server management
Now, lets look at hosting an application. The primary components of hosting are:
Well you need to build the application And you need the application to be hosted somewhere that keeps it running 24/7 You need to store the incoming user generated data somewhere, right. Once your application is up, you need to monitor and check for scaling of the application And last but not the least, you need to have separate access to specific users.</description></item><item><title>Python Automatic Libraries</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/python-automatic-libraries/</link><pubDate>Tue, 13 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/python-automatic-libraries/</guid><description>Automatic Machine Learning Libraries that ease your job pip install lazypredict Exploratory Data Analysis Pandas Profiling
Autoviz
Sweetviz
Machine Learning pip install lazypredict
NLP</description></item><item><title>Share Python with Packages</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/using-python-embeddable-to-deploy-locally/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/using-python-embeddable-to-deploy-locally/</guid><description>Install PIP and Packages in Python Embeddable Download Python Embeddable Python Embeddable is a lite flavor of python with barebone necessities. It can be used to deploy web-apps to systems which don&amp;rsquo;t have permission to install python or its packages.
[Download it from Here: Official Python 3.8.9](https://www.python.org/ftp/python/3.8.9/python-3.8.9-embed-amd64.zip)
Configure PIP Even if explicitly stated that the embeddable version of Python does not support Pip, it is possible with care. You need to:</description></item><item><title>Article Resources</title><link>https://do2blehelix.github.io/the-ml-handbook/information/article-resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/article-resources/</guid><description>Resources Here&amp;rsquo;s a list of useful resources
[Creating virtual Environments](https://towardsdatascience.com/virtual-environments-104c62d48c54) [Pip install in Python Embed](https://stackoverflow.com/a/48906746)</description></item><item><title>DBSCAN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</guid><description>Density Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN clusters densely packed points and labels the other points as noise.
Advantages No specification of no of clusters Flexibility in shapes and sizes of clusters Able to deal with noise and outliers Disadvantages Border points are assigned to whichever clusters find them first Faces difficulties in finding clusters of varying densities.
(variation of DBSCAN, HDBSCAN can be used to rectify) Theory Epsilon (ε) = Search Distance around a point</description></item><item><title>Gaussian Mixture Model</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</guid><description>Gaussian Mixture Model Clustering Each point belongs to every cluster, but has a different level of membership. It assumes that each cluster has a certain statistical distribution.
Expectation - Maximization Algorithm
Advantages : Soft Clustering, sample members of multiple clusters. Cluster shape flexibility. (cluster can contain another cluster inside of it) Disadvantages : Sensitive to initialization values Slow convergence rate Possible to converge to a local optimum Process: Initialize k Gaussian distributions Soft cluster the data into the Gaussian distributions (Expectation step) Re-estimate the Gaussian (Maximization step) Evaluate log-likelihood to check for convergence Repeat from Step.</description></item><item><title>KNN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</guid><description>KNN (K- Nearest Neighbors) It is a simple algorithm which classifies cases based on its votes by its nearest neighbors whose class is already known. The “K” is KNN algorithm is the number of nearest neighbors we wish to take vote from. The class to be chosen depends on a distance function.
Categorical : Euclidean, Manhattan, Minkowski
Continuous : Hamming
Advantages: Attributes with multiple missing values can be easily treated Can predict both qualitative &amp;amp; quantitative (by taking average) attributes Easy interpretation.</description></item><item><title>NLP</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/natural-language-processing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/natural-language-processing/</guid><description>Natural Language Processing NLP can be divided in supervised and unsupervised techniques
Supervised: Text Classification (Spam Recognition, labeling, etc) Spam Detection Sentiment Analysis Intent Classification Multi-Label, Multi-Class Text Classification Unsupervised: Topic Modeling Applications
Sentiment Analysis Speech Recognition Chatbot Machine Translation (Google Translate) Spell Checking Keyword Search Information Extraction Advertisement Matching NLU - Natural Language Understanding Mapping input to useful representations Analyzing different aspects of the language NLG - Natural Language Generation Text planning Sentence planning Text realization Ambiguities Lexical Ambiguity - Two or more possible meanings in a word.</description></item><item><title>Python Code Snippets</title><link>https://do2blehelix.github.io/the-ml-handbook/information/python-code-snippets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/python-code-snippets/</guid><description>Python Code Snippets</description></item><item><title>Statistical Tests</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</guid><description>Statistical Tests Statistical tests are conducted to accept or reject hypothesis. They can be divided into tests catering to specific distributions of data.
Parametric Test : used for normally distributed data (assess group means) Non-Parametric Test : used for skewed data (assess group medians) The tests are also classified according to their tails.
One Tailed : uni-directional (typing speed increases with more typing) Two tailed: bi-directional (typing speed can increase or decrease with more typing) ⚠ P-value is the probability of the sample means coming up equal to or even further away from the hypothesized population mean.</description></item><item><title>SVD</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</guid><description>Singular Value Decomposition</description></item><item><title>Time Series / Forecasting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</guid><description>Time Series (Forecasting) [Method = Box-Jenkins (B-J)] Components Trend : A long term (relatively smooth) pattern that usually persists for more than one year
Eg : increasing no of flight passengers
Seasonal : Pattern that occurs at regular intervals. [multiple times in a year (least is once a year)]
Eg December sale
Cyclical : Pattern that occurs over a long time, generally continues over a year
Eg : Recession
Random : The component obtained after the above patterns have been extracted.</description></item></channel></rss>