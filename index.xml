<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/</link><description>Recent content in Home on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Fri, 28 Feb 2020 10:08:56 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/index.xml" rel="self" type="application/rss+xml"/><item><title>May 2019</title><link>https://do2blehelix.github.io/the-ml-handbook/information/2019_may/</link><pubDate>Tue, 28 Jan 2020 00:10:51 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/2019_may/</guid><description>Markdown here</description></item><item><title>March 2019</title><link>https://do2blehelix.github.io/the-ml-handbook/information/2019_march/</link><pubDate>Tue, 28 Jan 2020 00:10:42 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/2019_march/</guid><description>Markdown here</description></item><item><title>February 2019</title><link>https://do2blehelix.github.io/the-ml-handbook/information/2019_february/</link><pubDate>Tue, 28 Jan 2020 00:10:37 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/2019_february/</guid><description>Markdown here</description></item><item><title>January 2019</title><link>https://do2blehelix.github.io/the-ml-handbook/information/2019_january/</link><pubDate>Tue, 28 Jan 2020 00:10:09 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/2019_january/</guid><description>Markdown here</description></item><item><title>Random Forest</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</guid><description>Random Forest Random Forest works as a large collection of decorrelated decision trees and is based on bagging (boosted aggregating).
A random sample is taken from the population with random variables (feature selection)A decision tree is made based on the sample.Multiple samples are taken with replacement (bootstrap sampling)Multiple decision trees are created based on their respective samplesAll the decision trees are used to create a ranking of classesThe final model is based on the most no votes for the class.</description></item><item><title>Logistic Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/1-logistic-regression/</link><pubDate>Wed, 21 Oct 2020 06:14:22 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/1-logistic-regression/</guid><description>Logistic Regression Method = Maximum Likelihood Estimation / Chi-square Logistic Regression is used to model the probability of an outcome. It is based on concept of Generalized Linear Model (GLM).
Dependent Variable Y = Binary (eg 0 or 1)
Independent Variable X = Continuous or Categorical
Since the dependent variable is binary, errors will be non-normally distributed. Also, Errors are heteroskedastic
Types: Binary / Dichotomous - Dependent Variable is binary in nature (eg.</description></item><item><title>Central Tendency</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/1-central-tendency/</guid><description>Measures of Central Tendency Mean The sum total of units divided by the number of units. Average
μ (population) | x̄ (sample) | 1/ni=1nxi
Median The middle / midpoint value in a sorted sequence. Middle
It divides the data 50(more):50(less).
Mode The most commonly occurring value Frequent
Two modes = Bimodal ; Multiple modes = Multimodal
  Q) Why do we use mean most times ?
A) It takes all values into consideration.</description></item><item><title>Linear Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</guid><description>Linear Regression Method = Ordinary Least Square{: .label } Dependent Variable Y = Continuous
Independent Variable X = Continuous
i.e the complete data should be numerical in nature
Objective is to minimize the sum of squares of the residuals
(residual=difference between observation and the fitted line)
Assumptions Linear relationship between dependent &amp;amp; independent variables No presence of outliers Independent variables are independent of each other (non colinear) Errors, also called residuals Should have constant variance (homoscedasticity) Are independent and identically distributed (iid) ie No Autocorrelation Are normally distributed with a mean of 0 Tests for Assumptions: Linearity Methods : Residuals vs Predicted plot / Residuals vs Actuals plot Corrections : Log transformation for strictly positive variables Adding regressor which is non-linear function eg x and x2 Create new variable which is sum/product of A &amp;amp; B Multicollinearity Methods: Correlation Matrix VIF (Variance Inflation Factor) VIF is calculated only on the Independent variables.</description></item><item><title>The Important Stuff Everyone Misses</title><link>https://do2blehelix.github.io/the-ml-handbook/information/important-stuff-everyone-misses/</link><pubDate>Tue, 28 Jan 2020 00:10:48 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/information/important-stuff-everyone-misses/</guid><description>Easy to Overlook these Multivariate Analysis is nothing but regression
Logistic regression IS a binomial regression (with logit link), a special case of the Generalized Linear Model. It doesn&amp;rsquo;t classify anything unless a threshold for the probability is set. Classification is just its application.
Stepwise regression is by no means a regression. It&amp;rsquo;s a (flawed) method of variable selection.
OLS is a method of estimation (among others: GLS, TLS, (RE)ML, PQL, etc.</description></item><item><title>Decision Trees</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/2-decision-trees/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/2-decision-trees/</guid><description>Decision Trees It&amp;rsquo;s a type of supervised learning algorithm mostly used for classification problems. Works for both categorical and continuous I/P O/P variables.
The population is split into two or more homogeneous sets based on the splitting criterion.
Analogy : They ask a bunch of questions to arrive at a particular answer. (20 Questions)
Types :
Categorical Variable Decision Tree : Categorical Target Variable Continuous Variable Decision Tree : Continuous Target Variable Terminology : Root Node : It represents the entire population / sample Parent and Child Node: A node, which is divided into sub-nodes is called parent node whereas sub-nodes are the child of parent node.</description></item><item><title>Gradient Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</guid><description>Gradient Boosting (GBM) &amp;amp; AdaBoost The base learner takes all the distributions and assign equal weight or attention to each observation.If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.
weight = ln ( accuracy / (1-accuracy) ) = ln (correctly classified / incorrectly classified)</description></item><item><title>Dispersion/Spread</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/2-dispersion-spread/</guid><description>Measures of Dispersion/Spread Name Spread Range Max - Min value Percentile Divides the data into 100 equal parts using 99 points Decile Divides the data into 10 equal parts using 9 points Quartile Divides the data into 4 equal parts using 3 points (25%, 50%, 75%) Median is the 2nd quartile (50%) . Interquartile range : 25% - 75% Boxplot uses a limit of 1.</description></item><item><title>Types of Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</guid><description>Types of Regression Simple Linear Regression It is the simples form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)
Multiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It as one Dependent Variable and Multiple Independent Variable.</description></item><item><title>Support Vector Machine</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/3-support-vector-machine/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/3-support-vector-machine/</guid><description>SVM (Support Vector Machine) ![](https://lh5.googleusercontent.com/n0CPoXbAgg0MNx5jxNNmn14h-aWrPgVDUj4uo_6DqnUL4iRX7ZHTjl8GoDwXn1IWbp1743NgaDXva8rDUtac5oKaPdAZMbJ4qaOqNx23JVCZHwEOwyeLdizmFHJG57oHKNidmboV =308x209)
In this algorithm, each data item is plotted as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate.
The aim is to determine the location of decision boundaries also known as hyperplane that produce the optimal separation of classes. Multiple frontiers are produced to suit the data.</description></item><item><title>XG Boost</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/3-xgboost/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/3-xgboost/</guid><description>XGBoost XGBoost stands for Xtreme Gradient Boosting. It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Can be used to solve regression, classification, ranking, and user-defined prediction problems. XGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. However, XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.</description></item><item><title>Distributions</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/3-distributions/</guid><description>Distributions Normal / Gaussian / Continuous Distribution data distributed symmetrically around the center skewness = kurtosis = 0. Mean = Median = Mode. Also known as the bell curve.
Binomial Distribution Discrete distribution used in statistics. Only counts 2 states typically 0 and 1.
Uniform Distribution Consists of similar values throughout
Skewed Distribution Data distributed which spikes towards either ends as opposed to the central spike in normal distribution (skewness: lack of symmetry)</description></item><item><title>Regularization</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</guid><description>Regularization Ridge Regression Lasso Regression</description></item><item><title>Naive Bayes</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/4-na%C3%AFve-bayes/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/4-na%C3%AFve-bayes/</guid><description>Naïve Bayes It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. P(c|x) is the posterior probability of class (target) given predictor (attribute). P(c) is the prior probability of class. P(x|c) is the likelihood which is the probability of predictor given class.</description></item><item><title>Covariance &amp; Correlation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/4-covariance-correlation/</guid><description>Covariance &amp;amp; Correlation Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people.
⚠️ Correlation doesn&amp;rsquo;t imply causation
Covariance : Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance.
Returns a value between -∞ &amp;amp; +∞
Sum [(X-u1) (Y-u2)] / (n-1)</description></item><item><title>Artificial Neural Network</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/5-artificial-neural-network/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/classification/5-artificial-neural-network/</guid><description>Artificial Neural Network (ANN) Artificial neural networks (ANNs) are types of computer architecture inspired by biological neural networks (Nervous systems of the brain) and are used to approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are presented as systems of interconnected “neurons” which can compute values from inputs and are capable of machine learning as well as pattern recognition due their adaptive natureAn artificial neural network operates by creating connections between many different processing elements each corresponding to a single neuron in a biological brain.</description></item><item><title>Hypothesis Testing</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/5-hypothesis-testing/</guid><description>Hypothesis Testing It the the action of declaring a hypothesis (a fundamental logical component) and proving its occurrence of proving its non-existence.
Types of Hypothesis Null Hypothesis H0 : μ1 = μ2x
The means of the two groups (μ1 &amp;amp; μ2) belong to the same population. (p &amp;gt; 0.05)
Alternative Hypothesis HA : μ1 ≠ μ2
The means of the two groups (μ1 &amp;amp; μ2) belong to different populations.</description></item><item><title>Markdown Syntax Guide</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/markdown-syntax/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/markdown-syntax/</guid><description>&lt;p>Lorem est tota propiore conpellat pectoribus de&lt;br />
pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice&lt;br />
subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc&lt;br />
caluere tempus&lt;/p></description></item><item><title>Rich Content</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/rich-content/</link><pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/rich-content/</guid><description>&lt;p>Lorem est tota propiore conpellat pectoribus de&lt;br />
pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice&lt;br />
subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc&lt;br />
caluere tempus&lt;/p></description></item><item><title>Placeholder Text</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/placeholder-text/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/placeholder-text/</guid><description>&lt;p>Lorem est tota propiore conpellat pectoribus de&lt;br />
pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice&lt;br />
subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc&lt;br />
caluere tempus&lt;/p></description></item><item><title>Emoji Support</title><link>https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/</link><pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/blog/emoji-support/</guid><description>&lt;p>Lorem est tota propiore conpellat pectoribus de&lt;br />
pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice&lt;br />
subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc&lt;br />
caluere tempus&lt;/p></description></item><item><title>DBSCAN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</guid><description>Density Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN clusters densely packed points and labels the other points as noise.
Advantages: No specification of no of clusters Flexibility in shapes and sizes of clusters Able to deal with noise and outliers Disadvantages: Border points are assigned to whichever clusters find them first Faces difficulties in finding clusters of varying densities.
(variation of DBSCAN, HDBSCAN can be used to rectify) Theory: Epsilon (ε) = Search Distance around a point</description></item><item><title>Gaussian Mixture Model</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</guid><description>Gaussian Mixture Model Clustering Each point belongs to every cluster, but has a different level of membership. It assumes that each cluster has a certain statistical distribution.
Expectation - Maximization Algorithm
Advantages : Soft Clustering, sample members of multiple clusters. Cluster shape flexibility. (cluster can contain another cluster inside of it) Disadvantages : Sensitive to initialization values Slow convergence rate Possible to converge to a local optimum Process: Initialize k Gaussian distributions Soft cluster the data into the Gaussian distributions (Expectation step) Re-estimate the Gaussian (Maximization step) Evaluate log-likelihood to check for convergence Repeat from Step.</description></item><item><title>Hierarchical Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</guid><description>Hierarchical Clustering It is a set of nested clusters organized as a hierarchical tree. No decision about number of clusters It is not used when data is big due to higher processing time.
Types:
Agglomerative : Start from n clusters and get to one cluster Bottom up approach Divisive : Start from one cluster and get to n clusters Top down approach Advantages: Produces an additional ability to visualize Potent esp if the data contains real hierarchical relationships (eg evolution) Disadvantages: Computationally intensive Sensitive to noise and outliers Distance Between Clusters (Agglomerative Clustering) Single link : Shortest distance between an element in one cluster and an element in another cluster.</description></item><item><title>K-Means</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</guid><description>K means [Non Hierarchical] It is based on division of objects into non overlapping subsets. Main objective is to form clusters that are homogeneous in nature and heterogeneous to each other.
❕ Only for continuous variables.
Advantages: Faster, more reliable, works with large data. Computationally lighter than other methods Disadvantages: Can only identify clusters circular / spherical in nature. (check crescent dataset) Distance based Process: Identify value of ‘k’ Assign random k observations as seeds Assign each record to one of the k seeds based on proximity Form clusters Calculate centroids of clusters Assign centroids as new seed Form new clusters Recalculate clusters Continue process until stable clusters are formed (boundary ceases to change) Elbow Criterion (Scree Plot):!</description></item><item><title>KNN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/knn-k-nearest-neighbors/</guid><description>KNN (K- Nearest Neighbors) It is a simple algorithm which classifies cases based on its votes by its nearest neighbors whose class is already known. The “K” is KNN algorithm is the number of nearest neighbors we wish to take vote from. The class to be chosen depends on a distance function.
Categorical : Euclidean, Manhattan, Minkowski
Continuous : Hamming
Advantages: Attributes with multiple missing values can be easily treated Can predict both qualitative &amp;amp; quantitative (by taking average) attributes Easy interpretation.</description></item><item><title>NLP</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/nlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/nlp/</guid><description>Natural Language Processing NLP can be divided in supervised and unsupervised techniques
Supervised: Text Classification (Spam Recognition, labeling, etc) Spam Detection Sentiment Analysis Intent Classification Multi-Label, Multi-Class Text Classification Unsupervised: Topic Modeling Applications
Sentiment Analysis Speech Recognition Chatbot Machine Translation (Google Translate) Spell Checking Keyword Search Information Extraction Advertisement Matching NLU - Natural Language Understanding Mapping input to useful representations Analyzing different aspects of the language NLG - Natural Language Generation Text planning Sentence planning Text realization Ambiguities Lexical Ambiguity - Two or more possible meanings in a word.</description></item><item><title>Others</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</guid><description>BIRCH</description></item><item><title>Spectral Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</guid><description>Spectral Clustering It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.</description></item><item><title>Statistical Tests</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/statistics/6-statistical-tests/</guid><description>Statistical Tests Statistical tests are conducted to accept or reject hypothesis. They can be divided into tests catering to specific distributions of data.
Parametric Test : used for normally distributed data (assess group means) Non-Parametric Test : used for skewed data (assess group medians) The tests are also classified according to their tails.
One Tailed : uni-directional (typing speed increases with more typing) Two tailed: bi-directional (typing speed can increase or decrease with more typing) ⚠ P-value is the probability of the sample means coming up equal to or even further away from the hypothesized population mean.</description></item><item><title>SVD</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/singular-value-decomposition/</guid><description>Singular Value Decomposition</description></item><item><title>Time Series / Forecasting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/time-series-forecasting/</guid><description>Time Series (Forecasting) [Method = Box-Jenkins (B-J)] Components Trend : A long term (relatively smooth) pattern that usually persists for more than one year
Eg : increasing no of flight passengers
Seasonal : Pattern that occurs at regular intervals. [multiple times in a year (least is once a year)]
Eg December sale
Cyclical : Pattern that occurs over a long time, generally continues over a year
Eg : Recession
Random : The component obtained after the above patterns have been extracted.</description></item><item><title>☑️ Cluster Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/cluster-validation/</guid><description>Cluster Validation Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item><item><title>☑️ Model Evaluation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/model-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/model-evaluation/</guid><description>Model Evaluation / Selection / Fits / Validation Train Test Split Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs. The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.</description></item></channel></rss>