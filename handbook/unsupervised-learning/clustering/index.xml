<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Clustering on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/</link><description>Recent content in Clustering on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/index.xml" rel="self" type="application/rss+xml"/><item><title>DBSCAN</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/dbscan/</guid><description>Density Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN clusters densely packed points and labels the other points as noise.
Advantages: No specification of no of clusters Flexibility in shapes and sizes of clusters Able to deal with noise and outliers Disadvantages: Border points are assigned to whichever clusters find them first Faces difficulties in finding clusters of varying densities.
(variation of DBSCAN, HDBSCAN can be used to rectify) Theory: Epsilon (ε) = Search Distance around a point</description></item><item><title>Gaussian Mixture Model</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/gaussian-mixture-model/</guid><description>Gaussian Mixture Model Clustering Each point belongs to every cluster, but has a different level of membership. It assumes that each cluster has a certain statistical distribution.
Expectation - Maximization Algorithm
Advantages : Soft Clustering, sample members of multiple clusters. Cluster shape flexibility. (cluster can contain another cluster inside of it) Disadvantages : Sensitive to initialization values Slow convergence rate Possible to converge to a local optimum Process: Initialize k Gaussian distributions Soft cluster the data into the Gaussian distributions (Expectation step) Re-estimate the Gaussian (Maximization step) Evaluate log-likelihood to check for convergence Repeat from Step.</description></item><item><title>Hierarchical Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/hierarchical-clustering/</guid><description>Hierarchical Clustering It is a set of nested clusters organized as a hierarchical tree. No decision about number of clusters It is not used when data is big due to higher processing time.
Types:
Agglomerative : Start from n clusters and get to one cluster Bottom up approach Divisive : Start from one cluster and get to n clusters Top down approach Advantages: Produces an additional ability to visualize Potent esp if the data contains real hierarchical relationships (eg evolution) Disadvantages: Computationally intensive Sensitive to noise and outliers Distance Between Clusters (Agglomerative Clustering) Single link : Shortest distance between an element in one cluster and an element in another cluster.</description></item><item><title>K-Means</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/k-means/</guid><description>K means (Non Hierarchical) from sklearn.cluster import KMeans It is based on division of objects into non overlapping subsets. Main objective is to form clusters that are homogeneous in nature and heterogeneous to each other.
❕ Only for continuous variables.
Advantages Faster, more reliable, works with large data. Computationally lighter than other methods Disadvantages Can only identify clusters circular / spherical in nature. (check crescent dataset) Distance based Process Identify value of ‘k’ Assign random k observations as seeds Assign each record to one of the k seeds based on proximity Form clusters Calculate centroids of clusters Assign centroids as new seed Form new clusters Recalculate clusters Continue process until stable clusters are formed (boundary ceases to change) Elbow Criterion (Scree Plot):!</description></item><item><title>Others</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/others/</guid><description>BIRCH</description></item><item><title>Spectral Clustering</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/spectral/</guid><description>Spectral Clustering It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well.</description></item><item><title>☑️ Cluster Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/unsupervised-learning/clustering/cluster-validation/</guid><description>Cluster Validation Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item></channel></rss>