<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Evaluation on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/</link><description>Recent content in Model Evaluation on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Wed, 21 Oct 2020 06:14:22 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/index.xml" rel="self" type="application/rss+xml"/><item><title>☑️ Regression Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</guid><description>Regression Metrics: MAE is more robust to data with outliers. The lower value of MAE, MSE, and RMSE implies higher accuracy of a regression model. However, a higher value of R square is considered desirable. R Squared &amp;amp; Adjusted R Squared are used for explaining how well the independent variables in the linear regression model explains the variability in the dependent variable. For comparing the accuracy among different linear regression models, RMSE is a better choice than R Squared.</description></item><item><title>☑️ Classification Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</guid><description>Classification Metrics : Confusion Matrix Actual (+) Actual (-) Predicted (+) TP FP Predicted (-) FN TN A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data
Misclassifications (notified on predicted value)
False Positive = detected positive, wrongly False Negative = detected negative, wrongly The most common evaluation metrics are as below: Precision TP ÷ (TP + FP)</description></item><item><title>☑️ Cluster Validation Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</guid><description>Cluster Validation Metrics Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item><item><title>Model Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</guid><description>Model Evaluation / Selection / Fits / Validation Train Test Split Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs. The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.</description></item><item><title>Model Choice</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</guid><description>Choice of Model Metrics Precision vs Recall Model Misclassifications (notified on predicted value)
False Positive = detected positive, wrongly False Negative = detected negative, wrongly Examples of wrong outcomes:
If an important mail is detected spam positive (FP), it is a bad outcome If a cancer patient is detected cancer negative (FN), it is a bad outcome We want to reduce the no of bad outcomes. To reduce the no of bad outcomes, we need the bad outcomes in the denominator.</description></item></channel></rss>