<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Evaluation on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/</link><description>Recent content in Model Evaluation on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Wed, 21 Oct 2020 06:14:22 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/index.xml" rel="self" type="application/rss+xml"/><item><title>☑️ Regression Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/regression-evaluation/</guid><description>Regression Metrics: R Square: % of variance in Y that is explained by X. It is defined as the square of correlation between Predicted and Actual values.
R2= SSEIndependent VarSSEIndependent Var + SSEErrors
Adjusted R Square: Similar to R2. It penalizes for adding impurity (insignificant variables) to the model
MSE (Mean Squared Error) : Sum of squares / degree of freedom
RMSE (Root Mean Square Error) : It measures standard deviation of the residuals.</description></item><item><title>☑️ Classification Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/supervised-model-evaluation/</guid><description>Classification Metrics : Confusion Matrix Actual (+) Actual (-) Predicted (+) TP FP Predicted (-) FN TN A confusion matrix shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data
Precision (aka PPV) : TP ÷ (TP + FP) Recall (aka Sensitivity) : TP ÷ (TP + FN) F1 Score : 2x (Precision*Recall) ÷ (Precision+Recall) Sensitivity : (True Positive Rate or Recall)</description></item><item><title>☑️ Cluster Validation Metrics</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/cluster-validation/</guid><description>Cluster Validation Metrics Validation is an important part of clustering and our end goal is to get the measure whether the clusters were efficient or not. Efficient clusters have similar data huddled together and have clear separation from other clusters.
Compactness : how close the data are to each other (within-cluster-variance)
Separability : how far/distinct clusters are from each other (between-cluster-variance)
Optimum clusters should have similar data clustered together (high compactness) and all clusters as far as possible from each other (high separability)</description></item><item><title>Model Validation</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-training-and-validation/</guid><description>Model Evaluation / Selection / Fits / Validation Train Test Split Before running the model, the data is split into Training and Testing sets. The training to test split ratio is generally 70:30 and can vary. The model is trained on the Training dataset hence the name Once the model is fit on the data, we use the Testing dataset to check how the model performs. The model is best served with a k-fold cross validation set where the data is randomly split multiple k times and each time the model fit is checked.</description></item><item><title>Model Choice</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/evaluation/model-choice.md/</guid><description>Choice of Model Metrics Precision vs Recall Model Precision
Bias vs Variance Model Bias - Variance Tradeoff
Bias : how much on average is my predicted values different from actual values. High bias is underfitting.
Variance : how different will predictions be, at the same point, if different samples are taken from the same population. High variance causes overfitting.
high bias = high (pred - act) = high error = underfit model</description></item></channel></rss>