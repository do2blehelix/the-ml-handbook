<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ensemble Learning on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/</link><description>Recent content in Ensemble Learning on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Sat, 03 Apr 2021 18:30:00 +0000</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/index.xml" rel="self" type="application/rss+xml"/><item><title>Bagging (Random Forest)</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</guid><description>Boosted Aggregating Bagging from sklearn.ensemble import BaggingClassifier Bagging is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.
Objective is to average noisy and unbiased models to create a model with low variance
Algorithm Create Multiple Datasets: Sampling is done with replacement on the original data and new datasets are formed. The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting Build Multiple Classifiers: Classifiers are built on each data set.</description></item><item><title>Gradient Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</guid><description>Boosting Algorithms The base learner takes all the distributions and assign equal weight or attention to each observation. If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm. Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved. weight = ln ( accuracy / (1-accuracy) ) = ln (correctly classified / incorrectly classified) The fundamental difference between Bagging and Random Forest is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.</description></item></channel></rss>