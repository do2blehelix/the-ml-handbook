<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ensemble Learning on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/</link><description>Recent content in Ensemble Learning on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Sat, 03 Apr 2021 18:30:00 +0000</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/index.xml" rel="self" type="application/rss+xml"/><item><title>Random Forest</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</guid><description>Random Forest Random Forest works as a large collection of decorrelated decision trees and is based on bagging (boosted aggregating).
A random sample is taken from the population with random variables (feature selection)A decision tree is made based on the sample.Multiple samples are taken with replacement (bootstrap sampling)Multiple decision trees are created based on their respective samplesAll the decision trees are used to create a ranking of classesThe final model is based on the most no votes for the class.</description></item><item><title>Gradient Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</guid><description>Gradient Boosting (GBM) &amp;amp; AdaBoost The base learner takes all the distributions and assign equal weight or attention to each observation.If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.
weight = ln ( accuracy / (1-accuracy) ) = ln (correctly classified / incorrectly classified)</description></item><item><title>XG Boost</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/3-xgboost/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/3-xgboost/</guid><description>XGBoost XGBoost stands for Xtreme Gradient Boosting. It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Can be used to solve regression, classification, ranking, and user-defined prediction problems. XGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. However, XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.</description></item></channel></rss>