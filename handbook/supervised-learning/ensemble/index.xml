<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ensemble Learning on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/</link><description>Recent content in Ensemble Learning on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Sat, 03 Apr 2021 18:30:00 +0000</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/index.xml" rel="self" type="application/rss+xml"/><item><title>Bagging (Random Forest)</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/1-random-forest/</guid><description>Boosted Aggregating Bagging from sklearn.ensemble import BaggingClassifier Bagging is an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.
Objective is to average noisy and unbiased models to create a model with low variance
Algorithm Create Multiple Datasets: Sampling is done with replacement on the original data and new datasets are formed. The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting Build Multiple Classifiers: Classifiers are built on each data set.</description></item><item><title>Boosting</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</link><pubDate>Sat, 03 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/2-gradient-boosting/</guid><description>Boosting The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.
It starts by assigning equal weights to each observation. Base learning algorithm is applied. If there is a prediction error, then the misclassified observations are assigned a higher weightage. Next base learning algorithm is applied. The iteration continues until the limit of learning algorithm is reached or higher accuracy is achieved. Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model.</description></item><item><title>Stacking</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</link><pubDate>Mon, 05 Apr 2021 18:30:00 +0000</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/ensemble/stacking/</guid><description>Stacking Stacking works in two phases.
First, we use multiple base classifiers to predict the class. Second, a new learner is used to combine their predictions with the aim of reducing the generalization error.</description></item></channel></rss>