<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Regression on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/</link><description>Recent content in Regression on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Wed, 21 Oct 2020 06:14:22 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/1-linear-regression/</guid><description>Linear Regression Method = Ordinary Least Square
Dependent Variable (the one you need to predict) Y = Continuous
Independent Variable (the others with which you will predict) X = Continuous
i.e the complete data should be numerical in nature
Objective is to minimize the sum of squares of the residuals
(residual=difference between observation and the fitted line)
Assumptions Linear relationship between dependent &amp;amp; independent variables No presence of outliers Independent variables are independent of each other (non colinear) Errors, also called residuals Should have constant variance (homoscedasticity) Are independent and identically distributed (iid) ie No Autocorrelation Are normally distributed with a mean of 0 Tests for Assumptions: Linearity Methods : Residuals vs Predicted plot / Residuals vs Actuals plot Corrections : Log transformation for strictly positive variables Adding regressor which is non-linear function eg x and x2 Create new variable which is sum/product of A &amp;amp; B Multicollinearity Methods: Correlation Matrix VIF (Variance Inflation Factor) VIF is calculated only on the Independent variables.</description></item><item><title>Types of Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/2-multiple-polynomial-regression/</guid><description>Types of Regression Simple Linear Regression It is the simples form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)
Multiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It as one Dependent Variable and Multiple Independent Variable.</description></item><item><title>Regularization</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/3-ridge-lasso-regression/</guid><description>Regularization Ridge Regression Lasso Regression</description></item></channel></rss>