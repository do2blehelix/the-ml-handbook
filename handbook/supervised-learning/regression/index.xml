<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Regression on TMLHB</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/</link><description>Recent content in Regression on TMLHB</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy;{year}, All Rights Reserved</copyright><lastBuildDate>Wed, 21 Oct 2020 06:14:22 +0900</lastBuildDate><atom:link href="https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/linear-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/linear-regression/</guid><description>Linear Regression Linear Regression is the most basic type of regression that there is. It takes a variable and states that on the basis of other variables, it will predict the concerned variable. It does this by simply drawing a line through the points and generating an equation.
Method = Ordinary Least Square
⚠️ Linear Regression requires the complete data should be numerical in nature.
Dependent Variable Y: the one you need to predict Independent Variable X: the others with which you will predict Residual: difference between observation and the fitted line Error: Residual The objective is to minimize the sum of squares of the residuals (difference between observation and the fitted line)</description></item><item><title>Types of Regression</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/multiple-polynomial-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/multiple-polynomial-regression/</guid><description>Types of Regression Simple Linear Regression It is the simplest form of Linear Regression where there is only one Dependent Variable and only one Independent Variable. The equation of the regression will simply be that of a straight line (y=mx+c)
Multiple Regression / (Multivariate Analysis) Multiple Regression is nothing but the most common form of regression which we do on a daily basis. It has one Dependent Variable and Multiple Independent Variables.</description></item><item><title>Regularization</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/ridge-lasso-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/ridge-lasso-regression/</guid><description>Regularization Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting
Ridge Regression (L2) Ridge regression is also called L2 regularization. It adds a constraint that is a linear function of the squared coefficients.
Lasso Regression (L1) Lasso is also known as L1 regularization. It penalizes the model by the absolute weight coefficients.
Elastic Net
Elastic Net is the combination of the L1 regularization and L2 regularization.</description></item><item><title>Other Algorithms</title><link>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/other-types-regression/</link><pubDate>Thu, 30 Jan 2020 00:38:25 +0900</pubDate><guid>https://do2blehelix.github.io/the-ml-handbook/handbook/supervised-learning/regression/other-types-regression/</guid><description>Various Algorithms for Regression As you will see later, there are many algorithms for classification eg decision trees, svm, etc. But what we ignore is that most of these algorithms pack in a regressor along with their classifier as well. Here&amp;rsquo;s some of the classification algorithms which are good fore regression as well.
Decision Tree Regressor from sklearn.tree import DecisionTreeRegressor Read about the classifier algorithm here
SVM Regressor from sklearn.</description></item></channel></rss>