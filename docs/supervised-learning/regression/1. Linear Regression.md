---
layout: default
title: Linear Regression
parent: Regression
grand_parent: Supervised Learning
nav_order: 1

---
### Linear Regression \[Method = Ordinary Least Square\]
___
&nbsp;

Y = Continuous ; X = Continuous

> Objective is to minimise the sum of squares of the residuals _(residual=difference between observation and the fitted line)_

&nbsp;


**Assumptions**
1. Linear relationship between dependent & independent variables
2. No presence of outliers
3. Independent variables are independent of each other (non collinear)
4. Errors, also called residuals
   1. Should have constant variance (homoscedasticity)
   2. Are independent and identically distributed (iid) ie No Autocorrelation
   3. Are normally distributed with a mean of 0

**Tests for Assumptions :**
* Linearity :
  * Methods :
    * Residuals vs Predicted plot / Residuals vs Actuals plot
  * Corrections :
    * Log transformation for strictly positive variables
    * Adding regressor which is non-linear function eg x and x2
    * Create new variable which is sum/product of A & B


![](https://lh4.googleusercontent.com/uPxT0fI8l2_jYFFCnzVlD05TKsX9T1Js5TCTW-Tc0oyh-vWcdGLV1BcS3jrBfR9sEIh8zh9V2TM455VcdzTLXQpi-LRELyR-ClwrwmeEiYbUrTUB16OC0PdmXc_W1x_QETnEKgpL)

* Multicollinearity
  * Methods:
    * Correlation Matrix
    * VIF (Variance Inflation Factor)

VIF is calculated only on the Independent variables. It runs a series of auxiliary regressions which fetches the R2 value of Xi against other IVs.

_Eg : If X2, X3, X4, have high R2 value when regressed against X1, it essentially means that X2, X3, X4 can explain a high amount of variation in X1 and it is redundant._ _Range = 1 to âˆž 1 < low < 5 < medium < 10 < high_


![](https://docs.google.com/drawings/u/0/d/saUG4xdKvHQGLUvZ395-ndA/image?w=149&h=168&rev=55&ac=1&parent=1DFQeQ9FRGL5QV2y-d85pI0R563x5zlvv_Ln5yTr6x-w)

* Homoscedasticity
  * Methods:
    * Goldfeld-Quandt test
    * Scatter plot (residuals vs predicted)
  * Corrections :
    * Take actual or predicted values of DV and plot it against errors. The plot should be random. If there is a trend, take log of DV.

* Autocorrelation
  * Durbin-Watson Test : Tests for serial correlations between errors

_Range : 0-4 positive < 2 (uncorrelated) < negative_

* Multivariate Normal
  * Methods:
    * Kolmogorov-Smirnov test / Shapiro-Wilk / Anderson-Darling / Jarque-Bera
    * Q-Q Plot
    * Histogram with fitted normal curve
  * Corrections:
    * Nonlinear / Log transformation

{{< hint danger>}}
**Dummy Variable Trap**
* Include one less variable when adding dummy variables to regression.
* The excluded variable serves as the base variable.
* All the other values are a reference to the base variable.
{{< /hint >}}
